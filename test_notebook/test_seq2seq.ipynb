{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting sequence\n",
    "\n",
    "### Grammar\n",
    "#### invert a sequence of numbers\n",
    "ex) '1', '2', '5', '2', '2', '6', '5', '1' -> '1', '5', '6', '2', '2', '5', '2', '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def invert_seq(data_size=5000):\n",
    "    dataset = []\n",
    "    for _ in range(data_size):\n",
    "        length = random.randint(3, 9)\n",
    "        seq = [str(random.randint(0, 9)) for _ in range(length)]\n",
    "        target = seq[::-1]\n",
    "        dataset.append((seq, target))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import Dictionary\n",
    "\n",
    "src_dict = Dictionary()\n",
    "tgt_dict = Dictionary(['<BOS>', '<EOS>'])\n",
    "for n in '0123456789':\n",
    "    src_dict.add_word(n)\n",
    "    tgt_dict.add_word(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = invert_seq(100)\n",
    "test = invert_seq(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Device =====\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from my_utils import DataLoader\n",
    "from torch_models.utils import seq2seq, get_device\n",
    "\n",
    "def numericalize(dataset, src_dict, tgt_dict):\n",
    "    numericalized = [([src_dict(s) for s in src], [tgt_dict(t) for t in tgt]) for src, tgt in dataset]\n",
    "    return numericalized\n",
    "\n",
    "device = get_device()\n",
    "train_loader = DataLoader(numericalize(train, src_dict, tgt_dict), batch_size=10, trans_func=seq2seq)\n",
    "test_loader = DataLoader(numericalize(test, src_dict, tgt_dict), batch_size=16, trans_func=seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_models.models import MLP, LSTMEncoder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, src_vocab_size, tgt_vocab_size, tgt_BOS, tgt_EOS, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = LSTMEncoder(embed_size, hidden_size, src_vocab_size, bidirectional=False, num_layers=num_layers)\n",
    "        self.decoder = LSTMEncoder(embed_size, hidden_size, tgt_vocab_size, bidirectional=False, num_layers=num_layers)\n",
    "        self.out_mlp = MLP(dims=[hidden_size, tgt_vocab_size])\n",
    "\n",
    "        self.tgt_BOS = tgt_BOS\n",
    "        self.tgt_EOS = tgt_EOS\n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        # encoding\n",
    "        _, enc_hiddens = self.encoder.forward(inputs)\n",
    "        # decoding\n",
    "        BOSs = torch.LongTensor([[self.tgt_BOS] for _ in range(len(inputs))])\n",
    "        decoded, _ = self.decoder.forward(BOSs, enc_hiddens)\n",
    "        decoded = decoded.squeeze(1)\n",
    "        out = self.out_mlp.forward(decoded)\n",
    "        return out\n",
    "\n",
    "    def generate(self, inputs, threshold=100):\n",
    "        # encoding\n",
    "        _, enc_hiddens = self.encoder.forward(inputs) # (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        generated = []\n",
    "        n_batch = enc_hiddens.shape[1]\n",
    "        for i in range(n_batch):\n",
    "            tgt_seq = []\n",
    "            current_token = self.tgt_BOS\n",
    "            hidden = enc_hiddens[:, i].unsqueeze(1)\n",
    "            for _ in range(threshold):\n",
    "                decoded, hidden = self.decoder.forward(torch.LongTensor([[current_token]]), hidden)\n",
    "                out = self.out_mlp.predict(decoded.squeeze(1)).item()\n",
    "                if out == self.tgt_EOS: break\n",
    "                tgt_seq.append(out)\n",
    "                current_token = out\n",
    "            generated.append(tgt_seq)\n",
    "        return generated\n",
    "\n",
    "\n",
    "    def fit(self, inputs, targets, optimizer):\n",
    "        # encoding\n",
    "        _, enc_hiddens = self.encoder.forward(inputs)\n",
    "        # decoding\n",
    "        BOS_targets = self._append_BOS(targets)\n",
    "        decoded, _ = self.decoder.forward(BOS_targets, enc_hiddens)\n",
    "        decoded = self._flatten_and_unpad(decoded)\n",
    "        # predicting\n",
    "        targets_EOS = self._append_EOS_flatten(targets)\n",
    "        loss = self.out_mlp.fit(decoded, targets_EOS, optimizer)\n",
    "        return loss\n",
    "\n",
    "    def _append_BOS(self, targets):\n",
    "        BOS_targets = [torch.cat((torch.tensor([self.tgt_BOS]), target)) for target in targets]\n",
    "        return BOS_targets\n",
    "        \n",
    "    def _append_EOS_flatten(self, targets):\n",
    "        EOS_targets = [torch.cat((target, torch.tensor([self.tgt_EOS]))) for target in targets]\n",
    "        return torch.cat(EOS_targets)\n",
    "    \n",
    "    def _flatten_and_unpad(self, decoded):\n",
    "        decoded = decoded.view(-1, self.decoder.hidden_size) # (batch * seq_len, embed_size)\n",
    "        decoded = torch.stack([tensor for tensor in decoded if not torch.tensor(float('-inf')) in tensor], dim=0)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): LSTMEncoder(\n",
      "    (embedding): Embedding(13, 13, padding_idx=12)\n",
      "    (rnn): LSTM(13, 300)\n",
      "  )\n",
      "  (decoder): LSTMEncoder(\n",
      "    (embedding): Embedding(13, 13, padding_idx=12)\n",
      "    (rnn): LSTM(13, 300)\n",
      "  )\n",
      "  (out_mlp): MLP(\n",
      "    (fc_out): Linear(in_features=300, out_features=12, bias=True)\n",
      "    (dropout): Dropout(p=0)\n",
      "    (criterion): CrossEntropyLoss()\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(13, 300, len(src_dict), len(tgt_dict),\n",
    "                tgt_BOS=tgt_dict('<BOS>'), tgt_EOS=tgt_dict('<EOS>'), num_layers=1)\n",
    "model.decoder.embedding.weight.data = torch.eye(13)\n",
    "model.encoder.embedding = model.decoder.embedding\n",
    "model.decoder.embedding.weight.requires_grad = False\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  \tloss: 2.4804214954376222\t\n",
      "epoch 1  \tloss: 2.4440106391906737\t\n",
      "epoch 2  \tloss: 2.3740649700164793\t\n",
      "epoch 3  \tloss: 2.239237356185913\t\n",
      "epoch 4  \tloss: 2.294962024688721\t\n",
      "epoch 5  \tloss: 2.3219133377075196\t\n",
      "epoch 6  \tloss: 2.333459210395813\t\n",
      "epoch 7  \tloss: 2.1924530982971193\t\n",
      "epoch 8  \tloss: 2.4210618495941163\t\n",
      "epoch 9  \tloss: 2.448764371871948\t\n",
      "epoch 10 \tloss: 2.3204408168792723\t\n",
      "epoch 11 \tloss: 2.2321518421173097\t\n",
      "epoch 12 \tloss: 2.1897303104400634\t\n",
      "epoch 13 \tloss: 2.1656079292297363\t\n",
      "epoch 14 \tloss: 2.1162369728088377\t\n",
      "epoch 15 \tloss: 2.116805815696716\t\n",
      "epoch 16 \tloss: 2.110946774482727\t\n",
      "epoch 17 \tloss: 2.0855512857437133\t\n",
      "epoch 18 \tloss: 2.073363208770752\t\n",
      "epoch 19 \tloss: 2.0471068143844606\t\n",
      "epoch 20 \tloss: 2.044719433784485\t\n",
      "epoch 21 \tloss: 2.0282179594039915\t\n",
      "epoch 22 \tloss: 2.008611631393433\t\n",
      "epoch 23 \tloss: 1.9876764297485352\t\n",
      "epoch 24 \tloss: 1.9866745948791504\t\n",
      "epoch 25 \tloss: 1.992501473426819\t\n",
      "epoch 26 \tloss: 1.9753814697265626\t\n",
      "epoch 27 \tloss: 1.9550715684890747\t\n",
      "epoch 28 \tloss: 1.937024426460266\t\n",
      "epoch 29 \tloss: 1.938696026802063\t\n",
      "epoch 30 \tloss: 1.8929460287094115\t\n",
      "epoch 31 \tloss: 1.905774188041687\t\n",
      "epoch 32 \tloss: 1.8867472887039185\t\n",
      "epoch 33 \tloss: 1.8631543159484862\t\n",
      "epoch 34 \tloss: 1.955497407913208\t\n",
      "epoch 35 \tloss: 1.9286144018173217\t\n",
      "epoch 36 \tloss: 1.9392913579940796\t\n",
      "epoch 37 \tloss: 1.8739753723144532\t\n",
      "epoch 38 \tloss: 1.8981765747070312\t\n",
      "epoch 39 \tloss: 1.896052098274231\t\n",
      "epoch 40 \tloss: 1.9249282121658324\t\n",
      "epoch 41 \tloss: 1.8636648416519166\t\n",
      "epoch 42 \tloss: 1.9004365444183349\t\n",
      "epoch 43 \tloss: 1.8126587629318238\t\n",
      "epoch 44 \tloss: 1.8130988359451294\t\n",
      "epoch 45 \tloss: 1.7634441137313843\t\n",
      "epoch 46 \tloss: 1.7943040370941161\t\n",
      "epoch 47 \tloss: 1.8474964857101441\t\n",
      "epoch 48 \tloss: 1.793036413192749\t\n",
      "epoch 49 \tloss: 1.7739412307739257\t\n",
      "epoch 50 \tloss: 1.7383351564407348\t\n",
      "epoch 51 \tloss: 1.6809808254241942\t\n",
      "epoch 52 \tloss: 1.6806310176849366\t\n",
      "epoch 53 \tloss: 1.6642278194427491\t\n",
      "epoch 54 \tloss: 1.6629087448120117\t\n",
      "epoch 55 \tloss: 1.6499655485153197\t\n",
      "epoch 56 \tloss: 1.6506532669067382\t\n",
      "epoch 57 \tloss: 1.6202415943145752\t\n",
      "epoch 58 \tloss: 1.6389716863632202\t\n",
      "epoch 59 \tloss: 1.6171868801116944\t\n",
      "epoch 60 \tloss: 1.5950913906097413\t\n",
      "epoch 61 \tloss: 1.5719055652618408\t\n",
      "epoch 62 \tloss: 1.592914342880249\t\n",
      "epoch 63 \tloss: 1.5824051380157471\t\n",
      "epoch 64 \tloss: 1.5887513160705566\t\n",
      "epoch 65 \tloss: 1.5427496671676635\t\n",
      "epoch 66 \tloss: 1.5426967144012451\t\n",
      "epoch 67 \tloss: 1.5089637517929078\t\n",
      "epoch 68 \tloss: 1.5413638830184937\t\n",
      "epoch 69 \tloss: 1.4934664011001586\t\n",
      "epoch 70 \tloss: 1.5043501377105712\t\n",
      "epoch 71 \tloss: 1.4682101249694823\t\n",
      "epoch 72 \tloss: 1.495146906375885\t\n",
      "epoch 73 \tloss: 1.4625918745994568\t\n",
      "epoch 74 \tloss: 1.5094469785690308\t\n",
      "epoch 75 \tloss: 1.4551462173461913\t\n",
      "epoch 76 \tloss: 1.4688438415527343\t\n",
      "epoch 77 \tloss: 1.4770172119140625\t\n",
      "epoch 78 \tloss: 1.4786436319351197\t\n",
      "epoch 79 \tloss: 1.460572862625122\t\n",
      "epoch 80 \tloss: 1.4095454454421996\t\n",
      "epoch 81 \tloss: 1.4358456969261169\t\n",
      "epoch 82 \tloss: 1.4734866976737977\t\n",
      "epoch 83 \tloss: 1.4340139508247376\t\n",
      "epoch 84 \tloss: 1.4100135684013366\t\n",
      "epoch 85 \tloss: 1.4684194564819335\t\n",
      "epoch 86 \tloss: 1.5397786140441894\t\n",
      "epoch 87 \tloss: 1.5349671602249146\t\n",
      "epoch 88 \tloss: 1.5309960842132568\t\n",
      "epoch 89 \tloss: 1.484844946861267\t\n",
      "epoch 90 \tloss: 1.4286367058753968\t\n",
      "epoch 91 \tloss: 1.4793641924858094\t\n",
      "epoch 92 \tloss: 1.4183203339576722\t\n",
      "epoch 93 \tloss: 1.4654322862625122\t\n",
      "epoch 94 \tloss: 1.4148660898208618\t\n",
      "epoch 95 \tloss: 1.4436932444572448\t\n",
      "epoch 96 \tloss: 1.4672633528709411\t\n",
      "epoch 97 \tloss: 1.4610920429229737\t\n",
      "epoch 98 \tloss: 1.40955992937088\t\n",
      "epoch 99 \tloss: 1.3868814349174499\t\n",
      "epoch 100\tloss: 1.4253498673439027\t\n",
      "epoch 101\tloss: 1.4309054613113403\t\n",
      "epoch 102\tloss: 1.3942035913467408\t\n",
      "epoch 103\tloss: 1.3977228641510009\t\n",
      "epoch 104\tloss: 1.4178409934043885\t\n",
      "epoch 105\tloss: 1.37990700006485\t\n",
      "epoch 106\tloss: 1.4139619946479798\t\n",
      "epoch 107\tloss: 1.4063404321670532\t\n",
      "epoch 108\tloss: 1.4008781909942627\t\n",
      "epoch 109\tloss: 1.403998351097107\t\n",
      "epoch 110\tloss: 1.3549050211906433\t\n",
      "epoch 111\tloss: 1.3711010456085204\t\n",
      "epoch 112\tloss: 1.358082914352417\t\n",
      "epoch 113\tloss: 1.392427670955658\t\n",
      "epoch 114\tloss: 1.3886546850204469\t\n",
      "epoch 115\tloss: 1.3809719324111938\t\n",
      "epoch 116\tloss: 1.3389644980430604\t\n",
      "epoch 117\tloss: 1.3471633791923523\t\n",
      "epoch 118\tloss: 1.369681978225708\t\n",
      "epoch 119\tloss: 1.3525277018547057\t\n",
      "epoch 120\tloss: 1.3978630781173706\t\n",
      "epoch 121\tloss: 1.4248944997787476\t\n",
      "epoch 122\tloss: 1.3840150237083435\t\n",
      "epoch 123\tloss: 1.4047378182411194\t\n",
      "epoch 124\tloss: 1.3551807761192323\t\n",
      "epoch 125\tloss: 1.3821848273277282\t\n",
      "epoch 126\tloss: 1.3576062321662903\t\n",
      "epoch 127\tloss: 1.3239100098609924\t\n",
      "epoch 128\tloss: 1.3602224230766295\t\n",
      "epoch 129\tloss: 1.334067165851593\t\n",
      "epoch 130\tloss: 1.3221366286277771\t\n",
      "epoch 131\tloss: 1.3729137420654296\t\n",
      "epoch 132\tloss: 1.3197502374649048\t\n",
      "epoch 133\tloss: 1.3286595106124879\t\n",
      "epoch 134\tloss: 1.2861334085464478\t\n",
      "epoch 135\tloss: 1.3543920516967773\t\n",
      "epoch 136\tloss: 1.2996838808059692\t\n",
      "epoch 137\tloss: 1.299692702293396\t\n",
      "epoch 138\tloss: 1.3029419779777527\t\n",
      "epoch 139\tloss: 1.2767198920249938\t\n",
      "epoch 140\tloss: 1.2980111956596374\t\n",
      "epoch 141\tloss: 1.2892675995826721\t\n",
      "epoch 142\tloss: 1.2608341693878173\t\n",
      "epoch 143\tloss: 1.266942811012268\t\n",
      "epoch 144\tloss: 1.228902781009674\t\n",
      "epoch 145\tloss: 1.2643421292304993\t\n",
      "epoch 146\tloss: 1.2753854870796204\t\n",
      "epoch 147\tloss: 1.2597875952720643\t\n",
      "epoch 148\tloss: 1.2573312878608705\t\n",
      "epoch 149\tloss: 1.2498191833496093\t\n",
      "epoch 150\tloss: 1.245512270927429\t\n",
      "epoch 151\tloss: 1.2417126297950745\t\n",
      "epoch 152\tloss: 1.1948904395103455\t\n",
      "epoch 153\tloss: 1.209951400756836\t\n",
      "epoch 154\tloss: 1.2016066908836365\t\n",
      "epoch 155\tloss: 1.1683640122413634\t\n",
      "epoch 156\tloss: 1.2030303120613097\t\n",
      "epoch 157\tloss: 1.1828881978988648\t\n",
      "epoch 158\tloss: 1.1870475888252259\t\n",
      "epoch 159\tloss: 1.175649094581604\t\n",
      "epoch 160\tloss: 1.1906176447868346\t\n",
      "epoch 161\tloss: 1.1572218894958497\t\n",
      "epoch 162\tloss: 1.1690274238586427\t\n",
      "epoch 163\tloss: 1.1554502725601197\t\n",
      "epoch 164\tloss: 1.143147611618042\t\n",
      "epoch 165\tloss: 1.1564275860786437\t\n",
      "epoch 166\tloss: 1.135607862472534\t\n",
      "epoch 167\tloss: 1.1439385771751405\t\n",
      "epoch 168\tloss: 1.1577049016952514\t\n",
      "epoch 169\tloss: 1.1794353246688842\t\n",
      "epoch 170\tloss: 1.1730813264846802\t\n",
      "epoch 171\tloss: 1.1445289373397827\t\n",
      "epoch 172\tloss: 1.1435668230056764\t\n",
      "epoch 173\tloss: 1.1481480717658996\t\n",
      "epoch 174\tloss: 1.096182405948639\t\n",
      "epoch 175\tloss: 1.1170220017433166\t\n",
      "epoch 176\tloss: 1.0933095455169677\t\n",
      "epoch 177\tloss: 1.127702260017395\t\n",
      "epoch 178\tloss: 1.0734845757484437\t\n",
      "epoch 179\tloss: 1.1045020818710327\t\n",
      "epoch 180\tloss: 1.062721312046051\t\n",
      "epoch 181\tloss: 1.0565908074378967\t\n",
      "epoch 182\tloss: 1.0533138513565063\t\n",
      "epoch 183\tloss: 1.0830214738845825\t\n",
      "epoch 184\tloss: 1.069779884815216\t\n",
      "epoch 185\tloss: 1.0914801836013794\t\n",
      "epoch 186\tloss: 1.0563846707344056\t\n",
      "epoch 187\tloss: 1.0869222283363342\t\n",
      "epoch 188\tloss: 1.0890188097953797\t\n",
      "epoch 189\tloss: 1.0725687861442565\t\n",
      "epoch 190\tloss: 1.0388975501060487\t\n",
      "epoch 191\tloss: 1.0132226705551148\t\n",
      "epoch 192\tloss: 1.014836072921753\t\n",
      "epoch 193\tloss: 1.0164266347885131\t\n",
      "epoch 194\tloss: 1.0483789086341857\t\n",
      "epoch 195\tloss: 1.0324406623840332\t\n",
      "epoch 196\tloss: 1.0452757954597474\t\n",
      "epoch 197\tloss: 1.0493104219436646\t\n",
      "epoch 198\tloss: 1.013336718082428\t\n",
      "epoch 199\tloss: 1.0216497778892517\t\n",
      "epoch 200\tloss: 1.0004401803016663\t\n",
      "epoch 201\tloss: 1.0023258924484253\t\n",
      "epoch 202\tloss: 1.0209127902984618\t\n",
      "epoch 203\tloss: 1.0190625667572022\t\n",
      "epoch 204\tloss: 0.9959334135055542\t\n",
      "epoch 205\tloss: 1.01981440782547\t\n",
      "epoch 206\tloss: 0.9833956956863403\t\n",
      "epoch 207\tloss: 1.0010699212551117\t\n",
      "epoch 208\tloss: 0.9851543724536895\t\n",
      "epoch 209\tloss: 1.0121903896331788\t\n",
      "epoch 210\tloss: 0.9940423488616943\t\n",
      "epoch 211\tloss: 0.9784326672554016\t\n",
      "epoch 212\tloss: 0.9979917645454407\t\n",
      "epoch 213\tloss: 0.9520671546459198\t\n",
      "epoch 214\tloss: 0.986062616109848\t\n",
      "epoch 215\tloss: 0.9955553591251374\t\n",
      "epoch 216\tloss: 0.9550148606300354\t\n",
      "epoch 217\tloss: 0.978278374671936\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-087a434ad1fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer.train(optimizer, max_epoch=1000,\n\u001b[0;32m---> 10\u001b[0;31m               evaluator=None, score_monitor=None, show_log=True, hook_func=None)\n\u001b[0m",
      "\u001b[0;32m/Users/linghan/my_modules/my_utils/my_utils/train_and_eval/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, max_epoch, evaluator, score_monitor, show_log, hook_func)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/my_modules/my_utils/my_utils/train_and_eval/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"epoch {:<3}\\tloss: {}\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-0a925e618681>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, targets, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtargets_EOS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_EOS_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_EOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/my_modules/torch_models/torch_models/models/mlp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, labels, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/anaconda/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from my_utils import Trainer, EvaluatorC\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "\n",
    "# evaluator = EvaluatorC(model, test_loader)\n",
    "\n",
    "trainer = Trainer(model, train_loader)\n",
    "trainer.train(optimizer, max_epoch=1000,\n",
    "              evaluator=None, score_monitor=None, show_log=True, hook_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter(train_loader)\n",
    "inputs, target = next(train_loader)\n",
    "generated = generate(model, inputs)\n",
    "print('======= input ======')\n",
    "for seq in inputs:\n",
    "    print([src_dict(s.item()) for s in seq])\n",
    "print('======= output ======')\n",
    "for seq in generated:\n",
    "    print([tgt_dict(s) for s in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, inputs, threshold=100):\n",
    "    # encoding\n",
    "    _, enc_hiddens = self.encoder.forward(inputs) # (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "    generated = []\n",
    "    n_batch = enc_hiddens.shape[1]\n",
    "    for i in range(n_batch):\n",
    "        tgt_seq = []\n",
    "        current_token = self.tgt_BOS\n",
    "        hidden = enc_hiddens[:, i].unsqueeze(1)\n",
    "        for _ in range(threshold):\n",
    "            decoded, hidden = self.decoder.forward(torch.LongTensor([[current_token]]), hidden)\n",
    "            out = self.out_mlp.predict(decoded.squeeze(1)).item()\n",
    "            if out == self.tgt_EOS: break\n",
    "            tgt_seq.append(out)\n",
    "            current_token = out\n",
    "        generated.append(tgt_seq)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dict(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
