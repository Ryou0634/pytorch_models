{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wArePITKUgQG"
   },
   "source": [
    "<img src=\"aiayn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSWEk4ttUgQH"
   },
   "source": [
    "> When teaching, I emphasize implementation as a way to understand recent developments in ML. This post is an attempt to keep myself honest along this goal. The recent [\"Attention is All You Need\"]\n",
    "(https://arxiv.org/abs/1706.03762) paper from NIPS 2017 has been instantly impactful paper as a new method for machine translation and potentiall NLP generally. The paper is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. \n",
    ">\n",
    "> In this post I follow the paper through from start to finish and try to implement each component in code. \n",
    "(I have done some minor reordering and skipping from the original paper). This document itself is a working notebook, and should be a completely usable and efficient implementation. To follow along you will first need to install [PyTorch](http://pytorch.org/) and [torchtext](https://github.com/pytorch/text). The complete code is available on [github](https://github.com/harvardnlp/annotated-transformer).\n",
    ">- Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1855,
     "output_extras": [
      {
       "item_id": 74
      },
      {
       "item_id": 108
      },
      {
       "item_id": 171
      },
      {
       "item_id": 193
      }
     ]
    },
    "colab_type": "code",
    "id": "ZaYyfFUqUnGY",
    "outputId": "f283707d-138b-4a18-8008-eb1857a88623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
      "  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 592.3MB 52.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.0.10.tar.gz (17.5MB)\n",
      "\u001b[K    31% |██████████▎                     | 5.6MB 35.0MB/s eta 0:00:01\u001b[K    100% |████████████████████████████████| 17.5MB 79kB/s \n",
      "\u001b[?25hCollecting torchtext\n",
      "  Downloading torchtext-0.2.1-py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib)\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2.tar.gz\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Collecting ftfy<5.0.0,>=4.4.2 (from spacy)\n",
      "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.7MB/s \n",
      "\u001b[?25hCollecting html5lib==1.0b8 (from spacy)\n",
      "  Downloading html5lib-1.0b8.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.3MB/s \n",
      "\u001b[?25hCollecting msgpack-numpy==0.4.1 (from spacy)\n",
      "  Downloading msgpack_numpy-0.4.1-py2.py3-none-any.whl\n",
      "Collecting msgpack-python==0.5.4 (from spacy)\n",
      "  Downloading msgpack-python-0.5.4.tar.gz\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy)\n",
      "  Downloading murmurhash-0.28.0.tar.gz\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.6MB/s \n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 7.7MB/s \n",
      "\u001b[?25hCollecting regex==2017.4.5 (from spacy)\n",
      "  Downloading regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K    100% |████████████████████████████████| 604kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Collecting thinc<6.11.0,>=6.10.1 (from spacy)\n",
      "  Downloading thinc-6.10.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.0MB/s \n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 5.6MB/s \n",
      "\u001b[?25hCollecting tqdm (from torchtext)\n",
      "  Downloading tqdm-4.19.9-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 9.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting cytoolz<0.9,>=0.8 (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "  Downloading cytoolz-0.8.2.tar.gz (386kB)\n",
      "\u001b[K    100% |████████████████████████████████| 389kB 2.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting wrapt (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "  Downloading wrapt-1.10.11.tar.gz\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "  Downloading toolz-0.9.0.tar.gz (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: spacy, cymem, ftfy, html5lib, msgpack-python, murmurhash, pathlib, preshed, regex, thinc, ujson, cytoolz, wrapt, toolz\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/a3/a3/c0/07e3ef38ab89f9339e73393d0fb04f5eccd1a0334d78b55b52\n",
      "  Running setup.py bdist_wheel for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/4b/2a/0e/dce3ff7a6f0f916906ef978afe512a7b5aef8abfd9ba988acf\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ae/d7/4c/339066248431397227741c7fdc80ad85826188ee9b0c24b4c7\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/d4/d1/0b/a6b6f9f204af55c9bb8c97eae2a78b690b7150a7b850bb9403\n",
      "  Running setup.py bdist_wheel for msgpack-python ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/49/fc/5d/07243edfc74536e0776f3f8901418e7596de0e3b887dd17d86\n",
      "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/67/30/e2/ef52a408eda4b23581669abfd1c1516a931d9e53f2e7616cb9\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/97/64/22/20fabf1f51039b799e64e46d0381b023cfdbe159c349d7c135\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/20/68/c71f468f76d9bd81730a7633ace0ad30507cf99166314109a1\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f8/fc/92/3bb08540cc5ac05df781005e686273adcd97af91ca2c032154\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
      "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/52/92/ed/661ecb7a67b42b21fc3dea140abb9ae9b8e94e72f0b3aff6c1\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/56/e1/0f/f7ccf1ed8ceaabccc2a93ce0481f73e589814cbbc439291345\n",
      "  Running setup.py bdist_wheel for toolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/57/51/8a/433a9c0a2c65fc1b2a795ae036b932f3339a02e9ae88367659\n",
      "Successfully built spacy cymem ftfy html5lib msgpack-python murmurhash pathlib preshed regex thinc ujson cytoolz wrapt toolz\n",
      "Installing collected packages: torch, cymem, html5lib, ftfy, msgpack-python, msgpack-numpy, murmurhash, pathlib, plac, preshed, regex, toolz, cytoolz, tqdm, wrapt, thinc, ujson, spacy, torchtext\n",
      "  Found existing installation: html5lib 0.9999999\n",
      "    Uninstalling html5lib-0.9999999:\n",
      "      Successfully uninstalled html5lib-0.9999999\n"
     ]
    }
   ],
   "source": [
    "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "4LTc4HW7UgQI"
   },
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_R749nLNUgQL"
   },
   "source": [
    "* Table of Contents                               \n",
    "{:toc}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esxhOQubUgQL"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1M-PiEMOUgQM"
   },
   "source": [
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
    "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
    "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
    "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
    "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
    "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
    "described in section 3.2.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
    "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
    "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned\n",
    "recurrence and have been shown to perform well on simple-language question answering and\n",
    "language modeling tasks [34].\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
    "entirely on self-attention to compute representations of its input and output without using sequencealigned\n",
    "RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
    "self-attention and discuss its advantages over models such as [17, 18] and [9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84vTAA5TUgQM"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-f9BuNsUgQN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBOvyU9BUgQN"
   },
   "source": [
    "Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](cho2014learning,bahdanau2014neural,sutskever14). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](graves2013generating), consuming the previously generated symbols as additional input when generating the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1AC8KeDJUgQO"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base model for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.encoder(self.src_embed(src), src_mask)\n",
    "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ip0EXqvEUgQQ"
   },
   "source": [
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9dznVhQUgQQ"
   },
   "source": [
    "<img src=\"ModalNet-21.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euyRXbaMUgQR"
   },
   "source": [
    "## Encoder and Decoder Stacks   \n",
    "\n",
    "### Encoder: \n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6yy7pY85UgQR"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "psiq5idJUgQT"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mie9sUeSUgQV"
   },
   "source": [
    "We employ a residual connection [(cite)](he2016deep) around each of the two sub-layers, followed by layer normalization [(cite)](layernorm2016).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sEz9kLClUgQV"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nx4On5PCUgQY"
   },
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout [(cite)](srivastava2014dropout) to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "zx9JBwAcUgQY"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity we apply the norm first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZduB6mIlUgQa"
   },
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0mEBw9tIUgQb"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHOZnpnBUgQd"
   },
   "source": [
    "### Decoder:\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3o_ZB42sUgQd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOi_W1qaUgQf"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kMm6xHWVUgQg"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nen9h7wUgQi"
   },
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RyQKI9AgUgQj"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 339,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1522639012913,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "fgQMtvM-UgQl",
    "outputId": "07611646-88f8-4410-8bd9-77bbe59b86cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a359d68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEyCAYAAACMONd1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXJ8JPUJMNPUpMMP0lN+j9S/8UQlmCK9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
    "# Words are blocked for attending to future words during training. \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl2E1IaqUgQq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3twSbimFUgQq"
   },
   "source": [
    "### Attention:                                                                                                                                                                                                                                                                               \n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.                                                                                                                                                                                                                                                                                           \n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.                                                                                                         \n",
    "<img width=\"220px\" src=\"ModalNet-19.png\">\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
    "                                                                 \n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$                                                                                                                                                                                                        \n",
    "                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wlZ8zw9PUgQr"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=0.0):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    # (Dropout described below)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV7cIqbrUgQs"
   },
   "source": [
    "The two most commonly used attention functions are additive attention [(cite)](bahdanau2014neural), and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.                                                                                             \n",
    "\n",
    "                                                                        \n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](DBLP:journals/corr/BritzGLL17). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OV7kNMbKUgQt"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiaCxaGGUgQt"
   },
   "source": [
    "### Multi-Head Attention                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.                                                                                                                                                                                                   \n",
    "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
    "\n",
    "<img width=\"270px\" src=\"ModalNet-20.png\">\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                                                                                                                                                                                                                                                                             \n",
    "    \n",
    "    \n",
    "   \n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                                                                                                                                                                                                                                        \n",
    "   \n",
    "\n",
    "   \n",
    "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_ea0UrEgUgQt"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVdpQ_KwUgQv"
   },
   "source": [
    "### Applications of Attention in our Model                                                                                                                                                      \n",
    "The Transformer uses multi-head attention in three different ways:                                                        \n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [(cite)](wu2016google, bahdanau2014neural,JonasFaceNet2017).    \n",
    "\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   \n",
    "\n",
    "\n",
    "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gERLhK-FUgQw"
   },
   "source": [
    "## Position-wise Feed-Forward Networks                                                                                                                                                                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                        \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HuDPthO2UgQx"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # Torch linears have a `b` by default. \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68VLwifsUgQz"
   },
   "source": [
    "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](press2016using). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sl5JzPeGUgQz"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_hw5TyCUgQ1"
   },
   "source": [
    "## Positional Encoding                                                                                                                             \n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](JonasFaceNet2017). \n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "$$                                                                                                                                                                                                                                                        \n",
    "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
    "                                                                                                                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MVsjhp6uUgQ1"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 321,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1522639038891,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "qMsBRCuLUgQ3",
    "outputId": "0ba52f87-93d6-441b-b6c6-b724981758db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAEyCAYAAACh2dIXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The positional encoding will add in a sine wave based on position.\n",
    "# The frequency and offset of the wave is different for each dimension.\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HjtmgH1UgQ5"
   },
   "source": [
    "We also experimented with using learned positional embeddings [(cite)](JonasFaceNet2017) instead, and found that the two versions produced nearly identical results.  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtnFnHH9UgQ6"
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "heaKRIaZUgQ6"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Standard generation step. (Not described in the paper.)\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0i97-Y7AUgQ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dd3lP9fTUgQ9"
   },
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "b-LDhRoaUgQ-"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model object based on hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3023,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1522639051120,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "qP-g4KfhUgQ_",
    "outputId": "e5d671a2-a9d4-461b-f08c-bf5b7a4ddb73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghan/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuV1e8nEUgRB"
   },
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OhVxtlZaUgRC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "naFpt_GUUgRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "X60DPvyaUgRF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz6n3REAUgRH"
   },
   "source": [
    "## Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [(cite)](wu2016google). \n",
    "\n",
    "\n",
    "Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PRoJURieUgRH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52xwbbb4UgRK"
   },
   "source": [
    "## Hardware and Schedule                                                                                                                                                                                                   \n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models, step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPp__T_uUgRK"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam optimizer [(cite)](kingma2014adam) with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We varied the learning rate over the course of training, according to the formula:                                                                                            \n",
    "$$                                                                                                                                                                                                                                                                                         \n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$                                                                                                                                                                                             \n",
    "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5yV0f2QUgRL"
   },
   "outputs": [],
   "source": [
    "# Note: This part is incredibly important. \n",
    "# Need to train with this setup of the model is very unstable.\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM5n1PJxUgRN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 266,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1522639068090,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "PC2wCVEFUgRO",
    "outputId": "b3fc5e15-3be4-4e1b-fb3b-fd722207a9a9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKEREJKn/B7iPc2wQRLNOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tkzxQYKUgRQ"
   },
   "source": [
    "## Regularization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                                                                                                                      \n",
    "### Label Smoothing\n",
    "\n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IVWDJLXrUgRQ"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1522639096676,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "_gXwt5HVUgRT",
    "outputId": "3ea1a660-0bc7-4b1e-ba8e-7d3fbf560640"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAADsCAYAAACcwaY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX85EOGiBmkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example\n",
    "crit = LabelSmoothing(5, 0, 0.5)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 288,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1522639115760,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "sHBHTwmjUgRU",
    "outputId": "4086db46-2dd9-4566-804e-8e5f80641a69"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: Index is supposed to be a vector at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524589329605/work/aten/src/TH/generic/THTensorMath.c:553",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-423c8a3db6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     return crit(Variable(predict.log()),\n\u001b[1;32m     10\u001b[0m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-423c8a3db6be>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     return crit(Variable(predict.log()),\n\u001b[1;32m     10\u001b[0m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-423c8a3db6be>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     return crit(Variable(predict.log()),\n\u001b[0;32m---> 10\u001b[0;31m                  Variable(torch.LongTensor([1]))).data[0]\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9aafb8117c4b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: Index is supposed to be a vector at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524589329605/work/aten/src/TH/generic/THTensorMath.c:553"
     ]
    }
   ],
   "source": [
    "# Label smoothing starts to penalize the model \n",
    "# if it gets very confident about a given choice\n",
    "crit = LabelSmoothing(5, 0, 0.2)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoyfFgLoUgRW"
   },
   "source": [
    "### Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yVKyONFsUgRW"
   },
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "LiTlbMq2UgRY"
   },
   "outputs": [],
   "source": [
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sSF9AaKJUgRZ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "q4kFYs7nUgRb"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.test()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "uJo5AZasUgRd"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "    \n",
    "def data_gen(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
    "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 604,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10550,
     "status": "error",
     "timestamp": 1522639142280,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "vrpU5b2sUgRe",
    "outputId": "00871f60-2fcb-4235-c0ab-8de02a0c069c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghan/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:17: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: Index is supposed to be a vector at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524589329605/work/aten/src/TH/generic/THTensorMath.c:553",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0666366b1d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-afe6365d6df7>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-720c52c40588>\u001b[0m in \u001b[0;36mloss_backprop\u001b[0;34m(generator, criterion, out, targets, normalize)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linghan/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-9aafb8117c4b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: Index is supposed to be a vector at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524589329605/work/aten/src/TH/generic/THTensorMath.c:553"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = get_std_opt(model)\n",
    "for epoch in range(2):\n",
    "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S2BcIoOUgRg"
   },
   "source": [
    "# A Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aP_oq0kLUgRh"
   },
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 989,
     "output_extras": [
      {
       "item_id": 12
      },
      {
       "item_id": 55
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25452,
     "status": "ok",
     "timestamp": 1522639211817,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "eKynVliVXg6F",
    "outputId": "f471f71a-d329-4fa9-d188-b4dcc9283234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-python==0.5.4 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: html5lib==1.0b8 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    6% |██                              | 2.4MB 47.2MB/s eta 0:00:01\u001b[K    100% |████████████████████████████████| 37.4MB 50.2MB/s \n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 38.2MB 50.0MB/s \n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 224145,
     "status": "ok",
     "timestamp": 1522639438247,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "lXtYwdHqUgRj",
    "outputId": "31ee7819-a5eb-4cf2-cb06-cc7932bf535a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading de-en.tgz\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
      ".data/iwslt/de-en/train.tags.de-en.en\n",
      ".data/iwslt/de-en/train.tags.de-en.de\n"
     ]
    }
   ],
   "source": [
    "# Load words from IWSLT\n",
    "\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN = 100\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 244,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 566,
     "status": "error",
     "timestamp": 1522639569474,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "F8MTIJTWUgRl",
    "outputId": "bd6c163a-14a2-4f8c-b0d5-01a033f278c4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a98b1f496512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Detail. Batching seems to matter quite a bit. \n",
    "# This is temporary code for dynamic batching based on number of tokens.\n",
    "# This code should all go away once things get merged in this library.\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
    "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
    "\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1900,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8732,
     "status": "error",
     "timestamp": 1522639447463,
     "user": {
      "displayName": "Sasha Rush",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "112736061112454937688"
     },
     "user_tz": 240
    },
    "id": "wamR3SPdUgRo",
    "outputId": "b3b92f29-2560-4dbb-c2b0-f2a0b71db37c"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b72405416f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         raise RuntimeError(\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Create the model an load it onto our GPU.\n",
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "SStCCZoiUgRp",
    "outputId": "a551e444-2fe5-4cd5-f46a-3de3505a2545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9.299771845340729 6.987712429686844e-07\n",
      "11 9.415135336574167 4.192627457812107e-06\n",
      "21 8.813630282878876 7.686483672655528e-06\n",
      "31 9.112178653478622 1.118033988749895e-05\n",
      "41 8.607461810112 1.4674196102342371e-05\n",
      "51 8.913826749660075 1.8168052317185794e-05\n",
      "61 8.701497752219439 2.1661908532029216e-05\n",
      "71 8.373274087905884 2.515576474687264e-05\n",
      "81 8.454237446188927 2.8649620961716057e-05\n",
      "91 7.6996782422065735 3.214347717655948e-05\n",
      "101 8.037408232688904 3.56373333914029e-05\n",
      "111 7.704962134361267 3.913118960624633e-05\n",
      "121 7.699015600606799 4.262504582108975e-05\n",
      "131 7.367554426193237 4.611890203593317e-05\n",
      "141 7.2071177661418915 4.961275825077659e-05\n",
      "151 7.106400920893066 5.310661446562001e-05\n",
      "161 6.804656069725752 5.660047068046343e-05\n",
      "171 6.390337720513344 6.0094326895306855e-05\n",
      "181 5.687528342008591 6.358818311015028e-05\n",
      "191 6.122820109128952 6.70820393249937e-05\n",
      "201 5.829070374369621 7.057589553983712e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NO9lsw2UgRt"
   },
   "source": [
    "\n",
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "B8BVm-hEUgRw"
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field()\n",
    "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 100\n",
    "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
    "                                    exts=('.en', '.fr'),\n",
    "                                    fields=(SRC, TGT), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "SRC.build_vocab(train.src, max_size=50000)\n",
    "TGT.build_vocab(train.trg, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "eFdZyOIzUgRx",
    "outputId": "bc543dba-c343-47bc-e81e-f74a25688668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50002, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50004, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=50004)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "print(pad_idx)\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
    "model_opt = get_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cinuTkbtUgRz"
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)\n",
    "    valid_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "PsoeJn4bUgR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "4LadFBIEUgR3",
    "outputId": "ba9a812f-b5f6-4972-af91-215d91a223d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50002\n"
     ]
    }
   ],
   "source": [
    "print(pad_idx)\n",
    "print(len(SRC.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "kP_Au0bHUgR7",
    "outputId": "d5ad5d88-d512-4947-b5b8-f18eaba4f9ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {},
      {},
      {},
      {},
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "nqKKIhoOUgR-",
    "outputId": "2c087978-9803-4639-886b-88df91e62096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.269582842476666 0.0005377044714644026\n",
      "101 3.300532897672383 0.0005726430336128369\n",
      "201 3.3047672072425485 0.0006075815957612711\n",
      "301 2.7151080842595547 0.0006425201579097052\n",
      "401 2.6975380268413574 0.0006774587200581396\n",
      "501 3.051631387323141 0.0007123972822065737\n",
      "601 2.554425454698503 0.000747335844355008\n",
      "701 2.6254820519825444 0.0007822744065034422\n",
      "801 2.868743653933052 0.0008172129686518764\n",
      "901 2.5978208642918617 0.0008521515308003106\n",
      "1001 2.5955790174775757 0.0008870900929487448\n",
      "1101 2.6764775353949517 0.000922028655097179\n",
      "1201 2.464000296778977 0.0009569672172456132\n",
      "1301 2.0503073083236814 0.0009919057793940475\n",
      "1401 2.295472824771423 0.0010268443415424816\n",
      "1501 2.245281406212598 0.0010617829036909158\n",
      "1601 2.2577588511630893 0.00109672146583935\n",
      "1701 2.2232908592559397 0.0011316600279877844\n",
      "1801 2.357596361427568 0.0011665985901362186\n",
      "1901 2.121352154412307 0.0012015371522846527\n",
      "2001 2.5742998471250758 0.001236475714433087\n",
      "2101 2.2518509055953473 0.0012714142765815214\n",
      "2201 2.2251326659170445 0.0013063528387299553\n",
      "2301 2.078994876006618 0.0013412914008783896\n",
      "2401 2.068276036065072 0.001376229963026824\n",
      "2501 2.31435151558253 0.0013907788851585368\n",
      "2601 1.9106871648691595 0.0013738752565588634\n",
      "2701 2.183084836578928 0.0013575733592730722\n",
      "2801 2.4668076275847852 0.0013418383196400342\n",
      "2901 1.963176985620521 0.0013266380295186675\n",
      "3001 2.2140520309330896 0.0013119428705609764\n",
      "3101 2.6989458349489723 0.0012977254713568687\n",
      "3201 2.1293521663174033 0.0012839604929174666\n",
      "3301 2.1402786187827587 0.0012706244386700126\n",
      "3401 2.041781216394156 0.0012576954857216498\n",
      "3501 2.051893091876991 0.0012451533346344698\n",
      "3601 1.5498304846696556 0.001232979075358713\n",
      "3701 2.763939742697403 0.001221155067309524\n",
      "3801 2.7611468499198963 0.0012096648318570434\n",
      "3901 1.7321470333263278 0.0011984929557393293\n",
      "4001 2.139603299088776 0.0011876250041103701\n",
      "4101 2.1966493157087825 0.0011770474421074978\n",
      "4201 2.0962203710805625 0.0011667475639689723\n",
      "4301 1.9717675620922819 0.0011567134288575545\n",
      "4401 2.097687987901736 0.0011469338026529508\n",
      "4501 1.9319786678534001 0.001137398105067946\n",
      "4601 1.8846281475271098 0.0011280963615221983\n",
      "4701 1.9817245414596982 0.0011190191592759865\n",
      "4801 1.7659185670199804 0.0011101576073853326\n",
      "4901 2.188665813198895 0.0011015033000912066\n",
      "5001 2.1391192222399695 0.0010930482833001135\n",
      "5101 1.8125874139368534 0.0010847850238522342\n",
      "5201 1.6616800595074892 0.0010767063813072288\n",
      "5301 1.6544548005331308 0.0010688055820075176\n",
      "5401 1.9542939933016896 0.0010610761952049212\n",
      "5501 2.218412609123334 0.0010535121110594244\n",
      "5601 1.838119359650591 0.001046107520339004\n",
      "5701 1.892627771012485 0.0010388568956672375\n",
      "5801 2.2462481096954434 0.0010317549741811346\n",
      "5901 1.4471426841337234 0.0010247967414755423\n",
      "6001 1.9312338004237972 0.0010179774167228303\n",
      "6101 1.7303275546291843 0.001011292438867507\n",
      "6201 1.8833909621462226 0.0010047374538051973\n",
      "6301 1.8943474531406537 0.0009983083024640838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5940000533591956 0.0009927515780513657\n",
      "101 1.7524283765815198 0.0009865483707369156\n",
      "201 1.900138527940726 0.0009804600111146078\n",
      "301 1.8419977760640904 0.0009744829985071481\n",
      "401 1.9621913449373096 0.0009686139798247046\n",
      "501 2.226916428655386 0.0009628497416600543\n",
      "601 1.7190162097394932 0.0009571872028951208\n",
      "701 1.8589106332874508 0.0009516234077802563\n",
      "801 1.8107321247807704 0.000946155519450957\n",
      "901 1.6531266793608665 0.0009407808138497059\n",
      "1001 1.4840005157748237 0.0009354966740233614\n",
      "1101 1.7578403616789728 0.0009303005847689719\n",
      "1201 1.3920216620899737 0.0009251901276031373\n",
      "1301 1.6626927084289491 0.0009201629760320567\n",
      "1401 1.7256765578058548 0.0009152168911012566\n",
      "1501 1.6049046433763579 0.0009103497172056578\n",
      "1601 1.6955451717367396 0.000905559378142174\n",
      "1701 1.6796367820352316 0.0009008438733884249\n",
      "1801 1.5794002648835885 0.0008962012745924116\n",
      "1901 1.9637197174597532 0.0008916297222591652\n",
      "2001 1.4656428614398465 0.0008871274226214399\n",
      "2101 1.567156056407839 0.0008826926446824871\n",
      "2201 1.542241255287081 0.0008783237174198395\n",
      "2301 1.690121710913445 0.0008740190271398465\n",
      "2401 1.357302049640566 0.0008697770149734477\n",
      "2501 1.9049871656461619 0.0008655961745043597\n",
      "2601 2.240402895025909 0.0008614750495214811\n",
      "2701 1.7940634173137369 0.0008574122318878972\n",
      "2801 1.7314323161263019 0.0008534063595194054\n",
      "2901 1.6064868164248765 0.0008494561144659686\n",
      "3001 1.7515187719254754 0.0008455602210899614\n",
      "3101 1.552100334316492 0.0008417174443354889\n",
      "3201 1.6221882179379463 0.0008379265880834463\n",
      "3301 1.5139061958470847 0.0008341864935873445\n",
      "3401 1.6668659402348567 0.0008304960379852562\n",
      "3501 2.1993618682026863 0.0008268541328835436\n",
      "3601 1.823760490231507 0.0008232597230083089\n",
      "3701 1.8189842144493014 0.0008197117849207771\n",
      "3801 1.689056838164106 0.0008162093257930558\n",
      "3901 1.5656833801185712 0.0008127513822409492\n",
      "4001 1.5621904337021988 0.0008093370192107105\n",
      "4101 1.4836799805052578 0.0008059653289168093\n",
      "4201 1.47899504378438 0.000802635429827976\n",
      "4301 1.6922758186701685 0.0007993464656989501\n",
      "4401 1.636858390578709 0.000796097604645519\n",
      "4501 1.5558803144613194 0.0007928880382605766\n",
      "4601 1.5102424336364493 0.00078971698076907\n",
      "4701 1.541241532890126 0.0007865836682198282\n",
      "4801 1.5931309935403988 0.000783487357712386\n",
      "4901 1.2315586884506047 0.0007804273266570247\n",
      "5001 1.527937745093368 0.0007774028720663579\n",
      "5101 1.31743333209306 0.0007744133098768835\n",
      "5201 1.5960889644484269 0.0007714579742990187\n",
      "5301 1.4181096099782735 0.0007685362171942096\n",
      "5401 1.4596448407392018 0.0007656474074777987\n",
      "5501 1.4594163084111642 0.0007627909305463981\n",
      "5601 1.62109798315214 0.0007599661877285873\n",
      "5701 1.586864550015889 0.0007571725957578231\n",
      "5801 1.5062829439411871 0.0007544095862665088\n",
      "5901 1.4292167258172412 0.0007516766053002225\n",
      "6001 1.4355267270930199 0.0007489731128511653\n",
      "6101 1.4162533966591582 0.0007462985824099354\n",
      "6201 1.6518787188415445 0.0007436525005347853\n",
      "6301 1.5916137372114463 0.0007410343664375577\n",
      "1 1.202994157327339 0.0007387531385993765\n",
      "101 1.4649722938484047 0.0007361862332058686\n",
      "201 1.1459896704182029 0.0007336459004644837\n",
      "301 1.417104929103516 0.0007311316850490442\n",
      "401 1.373963651509257 0.0007286431424819469\n",
      "501 1.6432027550181374 0.0007261798388040814\n",
      "601 1.4122836171882227 0.0007237413502569408\n",
      "701 1.6119428309611976 0.0007213272629763972\n",
      "801 1.5545603609643877 0.0007189371726976359\n",
      "901 1.5427279596333392 0.0007165706844707772\n",
      "1001 1.5437391183004365 0.0007142274123867243\n",
      "1101 1.9743895339342998 0.0007119069793128112\n",
      "1201 1.730805973522365 0.0007096090166378355\n",
      "1301 1.5635135210759472 0.0007073331640260875\n",
      "1401 1.206731209764257 0.000705079069180001\n",
      "1501 1.4495476994197816 0.0007028463876110714\n",
      "1601 1.2935033895773813 0.0007006347824187037\n",
      "1701 1.1734203454107046 0.000698443924076667\n",
      "1801 1.202259551268071 0.0006962734902268488\n",
      "1901 1.7874216835407424 0.0006941231654800159\n",
      "2001 1.5438914835685864 0.0006919926412233024\n",
      "2101 1.5168145569041371 0.000689881615434157\n",
      "2201 1.5306344364071265 0.0006877897925004977\n",
      "2301 1.5227781175635755 0.0006857168830468271\n",
      "2401 1.3308223116910085 0.0006836626037660786\n",
      "2501 1.4871021673316136 0.0006816266772569715\n",
      "2601 1.3415705130901188 0.0006796088318666611\n",
      "2701 1.2746119699440897 0.0006776088015384847\n",
      "2801 1.3439618053671438 0.0006756263256646049\n",
      "2901 1.3065503026737133 0.0006736611489433701\n",
      "3001 1.4918707825127058 0.0006717130212412112\n",
      "3101 1.4003087060991675 0.0006697816974589058\n",
      "3201 1.3473156996478792 0.0006678669374020495\n",
      "3301 1.3869949235959211 0.0006659685056555759\n",
      "3401 1.5086751837225165 0.000664086171462178\n",
      "3501 1.4735991460911464 0.00066221970860449\n",
      "3601 1.3997832712557283 0.0006603688952908887\n",
      "3701 1.5196008981074556 0.0006585335140447885\n",
      "3801 1.2834229312138632 0.0006567133515973014\n",
      "3901 1.3874705795169575 0.0006549081987831418\n",
      "4001 1.6422591609880328 0.0006531178504396635\n",
      "4101 1.305389653716702 0.0006513421053089143\n",
      "4201 1.5159487561322749 0.0006495807659426053\n",
      "4301 1.3981374967552256 0.0006478336386098913\n",
      "4401 1.7390631912276149 0.0006461005332078655\n",
      "4501 1.3604947600979358 0.0006443812631746732\n",
      "4601 1.7799529591429746 0.000642675645405156\n",
      "4701 1.3463407127128448 0.0006409835001689394\n",
      "4801 1.4632963918847963 0.0006393046510308797\n",
      "4901 1.1903231081087142 0.0006376389247737917\n",
      "5001 1.3287691511941375 0.0006359861513233783\n",
      "5101 1.3445309301023372 0.0006343461636752915\n",
      "5201 1.5431754024625661 0.0006327187978242499\n",
      "5301 1.3343850841192761 0.0006311038926951474\n",
      "5401 1.1768817943520844 0.0006295012900760858\n",
      "5501 1.6530805606771537 0.0006279108345532683\n",
      "5601 1.2646167293241888 0.000626332373447694\n",
      "5701 1.3651119051501155 0.000624765756753594\n",
      "5801 1.831987822048177 0.0006232108370785525\n",
      "5901 1.3451470380132378 0.0006216674695852594\n",
      "6001 1.5295006221767835 0.0006201355119348414\n",
      "6101 1.2796215488779126 0.0006186148242317232\n",
      "6201 1.3307579715619795 0.0006171052689699666\n",
      "6301 1.5296110774725094 0.0006156067109810445\n",
      "1 1.355640010209754 0.0006142969713181733\n",
      "101 1.3438594869803637 0.0006128187302418007\n",
      "201 1.3398014856502414 0.0006113511097561582\n",
      "301 1.2453488917089999 0.0006098939832926246\n",
      "401 1.74672898178801 0.0006084472263842588\n",
      "501 1.348103358541266 0.0006070107166211413\n",
      "601 1.2492765338683967 0.0006055843336068713\n",
      "701 1.568915182055207 0.0006041679589161831\n",
      "801 1.3617599749704823 0.0006027614760536461\n",
      "901 1.3296397840604186 0.0006013647704134199\n",
      "1001 1.506301498040557 0.0005999777292400283\n",
      "1101 1.1846984136500396 0.000598600241590126\n",
      "1201 1.1235853107646108 0.0005972321982952243\n",
      "1301 1.3506322290195385 0.0005958734919253515\n",
      "1401 1.5431637589354068 0.0005945240167536175\n",
      "1501 1.4227895765798166 0.0005931836687216574\n",
      "1601 1.2444980588334147 0.0005918523454059284\n",
      "1701 1.37204463215312 0.000590529945984835\n",
      "1801 1.3662666375166737 0.0005892163712066582\n",
      "1901 1.758998476434499 0.0005879115233582672\n",
      "2001 1.3996043455335894 0.0005866153062345879\n",
      "2101 1.409632071852684 0.0005853276251088103\n",
      "2201 1.3139934270293452 0.0005840483867033116\n",
      "2301 1.2863777373568155 0.0005827774991612753\n",
      "2401 1.1966209802776575 0.0005815148720189864\n",
      "2501 1.3174833165830933 0.0005802604161787846\n",
      "2601 1.406668136944063 0.0005790140438826557\n",
      "2701 1.31760111481708 0.0005777756686864456\n",
      "2801 1.22686495014932 0.0005765452054346768\n",
      "2901 1.4871160766715548 0.0005753225702359537\n",
      "3001 1.3321835576352896 0.0005741076804389384\n",
      "3101 1.349290698301047 0.0005729004546088824\n",
      "3201 1.0498975263908505 0.0005717008125046992\n",
      "3301 1.4295434548403136 0.0005705086750565621\n",
      "3401 1.3862976277887356 0.0005693239643440145\n",
      "3501 1.3612052928074263 0.0005681466035745775\n",
      "3601 1.3539716337691061 0.0005669765170628427\n",
      "3701 1.3053378225304186 0.0005658136302100359\n",
      "3801 1.2067344364186283 0.0005646578694840415\n",
      "3901 1.417662046442274 0.0005635091623998715\n",
      "4001 1.2578378450434684 0.0005623674375005725\n",
      "4101 1.2363171717152 0.0005612326243385544\n",
      "4201 1.3426340871083084 0.0005601046534573332\n",
      "4301 1.3097076122212457 0.000558983456373675\n",
      "4401 1.0131576862186193 0.0005578689655601316\n",
      "4501 1.4332989812392043 0.000556761114427959\n",
      "4601 1.4043821960221976 0.0005556598373104054\n",
      "4701 1.373746110650245 0.0005545650694463629\n",
      "4801 1.2657524709356949 0.0005534767469643717\n",
      "4901 1.1224889098666608 0.0005523948068669684\n",
      "5001 1.2615516305086203 0.000551319187015369\n",
      "5101 1.409785834257491 0.0005502498261144795\n",
      "5201 1.3791224808810512 0.0005491866636982242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301 1.2408291140163783 0.0005481296401151859\n",
      "5401 1.3008261130889878 0.0005470786965145471\n",
      "5501 1.1700160388209042 0.0005460337748323287\n",
      "5601 1.2999350049067289 0.000544994817777915\n",
      "5701 1.3322585223941132 0.0005439617688208604\n",
      "5801 1.254337038320955 0.0005429345721779703\n",
      "5901 1.773689029644629 0.000541913172800649\n",
      "6001 1.3898115772462916 0.0005408975163625087\n",
      "6101 1.4735579792177305 0.0005398875492472326\n",
      "6201 1.05738219874911 0.000538883218536687\n",
      "6301 1.0802461032290012 0.0005378844719992749\n",
      "1 1.286231731530279 0.0005370101533168812\n",
      "101 1.2250633136718534 0.0005360217659787991\n",
      "201 1.239320948603563 0.0005350388161199592\n",
      "301 1.4140636462761904 0.0005340612540665886\n",
      "401 1.442663955502212 0.0005330890307779102\n",
      "501 1.4505203103472013 0.0005321220978358095\n",
      "601 1.2115196966333315 0.0005311604074347066\n",
      "701 1.2035035027656704 0.0005302039123716286\n",
      "801 1.3747974793659523 0.0005292525660364788\n",
      "901 1.36490419106849 0.0005283063224024965\n",
      "1001 1.1864821948111057 0.0005273651360169036\n",
      "1101 1.1623371304303873 0.000526428961991735\n",
      "1201 1.1043747729854658 0.0005254977559948457\n",
      "1301 1.6982813560443901 0.0005245714742410941\n",
      "1401 1.2719842366641387 0.0005236500734836944\n",
      "1501 1.2951120301149786 0.0005227335110057353\n",
      "1601 1.580276207998395 0.0005218217446118628\n",
      "1701 1.218743062199792 0.0005209147326201215\n",
      "1801 1.1479590674862266 0.0005200124338539494\n",
      "1901 1.2872504810075043 0.0005191148076343284\n",
      "2001 1.8993003838438653 0.0005182218137720798\n",
      "2101 1.2762204335303977 0.0005173334125603075\n",
      "2201 1.6183682525045242 0.0005164495647669814\n",
      "2301 1.2522982619411778 0.0005155702316276618\n",
      "2401 1.2925108795752749 0.0005146953748383575\n",
      "2501 1.340747339767404 0.0005138249565485178\n",
      "2601 1.340512964350637 0.0005129589393541545\n",
      "2701 1.1672844442073256 0.0005120972862910908\n",
      "2801 1.257948145037517 0.0005112399608283344\n",
      "2901 1.510728154462413 0.0005103869268615725\n",
      "3001 1.4130934766726568 0.0005095381487067851\n",
      "3101 1.2367545471934136 0.0005086935910939762\n",
      "3201 1.3846962348325178 0.0005078532191610173\n",
      "3301 1.2582954101526411 0.0005070169984476032\n",
      "3401 1.1545094328466803 0.0005061848948893172\n",
      "3501 1.295005505089648 0.0005053568748118022\n",
      "3601 1.3319955187034793 0.0005045329049250373\n",
      "3701 1.3548947679810226 0.000503712952317716\n",
      "3801 1.4635376840888057 0.0005028969844517252\n",
      "3901 1.6542128916307774 0.0005020849691567213\n",
      "4001 1.3512894048908493 0.0005012768746248036\n",
      "4101 1.397591198408918 0.0005004726694052806\n",
      "4201 1.3055676214280538 0.0004996723223995292\n",
      "4301 1.3375271083787084 0.000498875802855943\n",
      "4401 1.2366086341207847 0.0004980830803649704\n",
      "4501 1.2439679206581786 0.0004972941248542376\n",
      "4601 1.352382222772576 0.0004965089065837576\n",
      "4701 1.7570512742054234 0.0004957273961412208\n",
      "4801 1.232903058291413 0.0004949495644373684\n",
      "4901 1.015858386293985 0.0004941753827014446\n",
      "5001 1.381107110035373 0.000493404822476726\n",
      "5101 0.9564947709441185 0.0004926378556161293\n",
      "5201 1.228621664486127 0.0004918744542778926\n",
      "5301 1.182083563413471 0.0004911145909213302\n",
      "5401 1.2583643229590962 0.0004903582383026592\n",
      "5501 1.404046923678834 0.0004896053694708976\n",
      "5601 1.2389367091745953 0.0004888559577638302\n",
      "5701 1.119320425321348 0.00048810997680404295\n",
      "5801 1.586507015679672 0.0004873674004950231\n",
      "5901 1.112720330056618 0.0004866282030173253\n",
      "6001 1.3577893248293549 0.0004858923588248005\n",
      "6101 1.217524498468265 0.0004851598426408882\n",
      "6201 1.3229771983387764 0.0004844306294549693\n",
      "6301 1.5693217546272535 0.0004837046945187796\n",
      "1 1.1786362157727126 0.00048306134975017534\n",
      "101 1.28241519164294 0.00048234154403106603\n",
      "201 1.1411214591062162 0.00048162494648183897\n",
      "301 1.2352831599419005 0.0004809115333417623\n",
      "401 1.1032181181944907 0.00048020128109574806\n",
      "501 1.18390864826506 0.00047949416647109663\n",
      "601 1.2226583541632863 0.00047879016643429347\n",
      "701 1.0373018080717884 0.0004780892581878584\n",
      "801 1.2819566036341712 0.00047739141916724456\n",
      "901 1.1648676298791543 0.0004766966270377871\n",
      "1001 1.1654199322802015 0.00047600485969170105\n",
      "1101 1.2386636545270449 0.00047531609524512704\n",
      "1201 1.2253044219105504 0.0004746303120352227\n",
      "1301 1.375744077755371 0.0004739474886173019\n",
      "1401 1.1551300736318808 0.0004732676037620178\n",
      "1501 1.5255512128351256 0.00047259063645259034\n",
      "1601 1.255034319277911 0.00047191656588207824\n",
      "1701 1.1623500876303297 0.0004712453714506923\n",
      "1801 1.2958592986833537 0.00047057703276315175\n",
      "1901 1.1341320046922192 0.0004699115296260807\n",
      "2001 1.1937441515619867 0.0004692488420454462\n",
      "2101 1.7062073841661913 0.00046858895022403485\n",
      "2201 1.2566360468044877 0.00046793183455896863\n",
      "2301 1.2216275975806639 0.0004672774756392595\n",
      "2401 1.2636524712725077 0.0004666258542434008\n",
      "2501 1.2113699619076215 0.00046597695133699556\n",
      "2601 1.1559934263350442 0.00046533074807042176\n",
      "2701 1.256740387296304 0.0004646872257765319\n",
      "2801 1.3039579528664262 0.0004640463659683885\n",
      "2901 1.2651300196012016 0.0004634081503370334\n",
      "3001 1.2652980692801066 0.000462772560749291\n",
      "3101 1.1218284339411184 0.00046213957924560355\n",
      "3201 1.2543016897689085 0.0004615091880379007\n",
      "3301 1.2131407480192138 0.0004608813695074994\n",
      "3401 1.2994702684518415 0.0004602561062030357\n",
      "3501 1.2115506358095445 0.00045963338083842724\n",
      "3601 1.1760960748360958 0.00045901317629086643\n",
      "3701 1.0682971130590886 0.00045839547559884254\n",
      "3801 1.0764332090620883 0.00045778026196019347\n",
      "3901 1.1835216325707734 0.0004571675187301866\n",
      "4001 1.3529939632862806 0.00045655722941962654\n",
      "4101 1.3684578015236184 0.0004559493776929923\n",
      "4201 1.2233722301607486 0.0004553439473666001\n",
      "4301 1.2596116681525018 0.0004547409224067939\n",
      "4401 1.2757911044172943 0.00045414028692816196\n",
      "4501 1.2199301174841821 0.0004535420251917793\n",
      "4601 1.3471774608151463 0.0004529461216034753\n",
      "4701 1.475795219448628 0.0004523525607121267\n",
      "4801 1.1835241899825633 0.0004517613272079745\n",
      "4901 1.1791616377497576 0.0004511724059209659\n",
      "5001 1.3126113665202865 0.0004505857818191191\n",
      "5101 1.2516068609402282 0.0004500014400069121\n",
      "5201 1.178165558274486 0.0004494193657236937\n",
      "5301 1.6013869942435122 0.0004488395443421177\n",
      "5401 1.2677101592962572 0.00044826196136659916\n",
      "5501 1.1976667390699731 0.0004476866024317922\n",
      "5601 1.1990807302790927 0.00044711345330108884\n",
      "5701 1.1415361673789448 0.0004465424998651406\n",
      "5801 1.2389779405202717 0.0004459737281403985\n",
      "5901 1.1746156329172663 0.0004454071242676752\n",
      "6001 1.1718775559565984 0.0004448426745107265\n",
      "6101 1.1669323876558337 0.00044428036525485275\n",
      "6201 1.22836275130976 0.0004437201830055194\n",
      "6301 1.1068585112225264 0.000443162114386997\n",
      "1 1.1908240653865505 0.00044267275186678196\n",
      "101 1.156728027795907 0.0004421186210736662\n",
      "201 1.151486962888157 0.0004415665660409348\n",
      "301 1.1075408830074593 0.0004410165738412884\n",
      "401 1.1251853418070823 0.00044046863165985925\n",
      "501 1.224421168473782 0.0004399227267929559\n",
      "601 1.1097798637929372 0.00043937884664682695\n",
      "701 0.992531725903973 0.0004388369787364407\n",
      "801 1.2762772621936165 0.0004382971106842813\n",
      "901 1.154728337773122 0.00043775923021916087\n",
      "1001 0.9699444866273552 0.00043722332517504866\n",
      "1101 1.1039727496681735 0.0004366893834899152\n",
      "1201 1.2997219555545598 0.0004361573932045913\n",
      "1301 1.5713044246076606 0.00043562734246164385\n",
      "1401 1.1782071397465188 0.00043509921950426545\n",
      "1501 1.256332863289117 0.0004345730126751789\n",
      "1601 1.162631830346072 0.00043404871041555687\n",
      "1701 1.1123517343075946 0.00043352630126395546\n",
      "1801 1.0946980192093179 0.0004330057738552615\n",
      "1901 1.120711475959979 0.0004324871169196544\n",
      "2001 1.1385652619646862 0.0004319703192815812\n",
      "2101 1.0391206528292969 0.0004314553698587452\n",
      "2201 1.1468603002722375 0.00043094225766110786\n",
      "2301 1.1944863148819422 0.0004304309717899036\n",
      "2401 1.1445480604888871 0.00042992150143666746\n",
      "2501 1.160092411795631 0.0004294138358822756\n",
      "2601 0.905779943568632 0.00042890796449599795\n",
      "2701 1.2337692737637553 0.0004284038767345632\n",
      "2801 1.2654334787439439 0.00042790156214123586\n",
      "2901 1.2613030684588011 0.0004274010103449054\n",
      "3001 1.1566388571663992 0.0004269022110591865\n",
      "3101 1.1506170178181492 0.0004264051540815317\n",
      "3201 1.1042177192866802 0.00042590982929235444\n",
      "3301 1.268968387885252 0.00042541622665416415\n",
      "3401 1.1708880871301517 0.0004249243362107117\n",
      "3501 1.1094016103306785 0.00042443414808614573\n",
      "3601 1.3188527839665767 0.0004239456524841804\n",
      "3701 1.2144307589042 0.0004234588396872726\n",
      "3801 1.1827894128946355 0.0004229737000558104\n",
      "3901 0.9924444004427642 0.00042249022402731095\n",
      "4001 1.1228576390712988 0.0004220084021156294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101 1.1924936635477934 0.0004215282249101765\n",
      "4201 1.1275967326655518 0.0004210496830751471\n",
      "4301 1.0625419117277488 0.0004205727673487576\n",
      "4401 1.1389823842037003 0.00042009746854249313\n",
      "4501 1.339291847194545 0.00041962377754036395\n",
      "4601 1.0302886090357788 0.0004191516852981713\n",
      "4701 1.7122778899138211 0.00041868118284278167\n",
      "4801 1.2910672437865287 0.0004182122612714111\n",
      "4901 1.0494152382598259 0.00041774491175091685\n",
      "5001 1.1782474033534527 0.0004172791255170995\n",
      "5101 1.040663594380021 0.0004168148938740118\n",
      "5201 0.9901199785526842 0.00041635220819327733\n",
      "5301 1.5490817801910453 0.00041589105991341656\n",
      "5401 1.1346296942792833 0.00041543144053918197\n",
      "5501 1.223581779631786 0.00041497334164089994\n",
      "5601 0.958975835936144 0.0004145167548538224\n",
      "5701 1.238148811913561 0.0004140616718774844\n",
      "5801 1.1881117207813077 0.000413608084475071\n",
      "5901 1.1295715225860476 0.0004131559844727907\n",
      "6001 1.0610488005331717 0.000412705363759257\n",
      "6101 1.2371235750615597 0.00041225621428487707\n",
      "6201 1.1479767516138963 0.00041180852806124783\n",
      "6301 1.2059640220250003 0.0004113622971605593\n",
      "1 1.1765662879188312 0.0004109663692823915\n",
      "101 1.219779463717714 0.0004105228675028437\n",
      "201 0.972488499362953 0.00041008079847008953\n",
      "301 1.1617506150214467 0.0004096401544864815\n",
      "401 1.0883347367926035 0.0004092009279121472\n",
      "501 1.1085488446406089 0.00040876311116443343\n",
      "601 1.1023443718672752 0.0004083266967173559\n",
      "701 1.018611608800711 0.00040789167710105623\n",
      "801 1.1658196483003849 0.00040745804490126497\n",
      "901 1.2917855954219704 0.0004070257927587706\n",
      "1001 1.186474629881559 0.00040659491336889525\n",
      "1101 1.127356821874855 0.00040616539948097586\n",
      "1201 1.1975307842949405 0.00040573724389785204\n",
      "1301 1.1174790017685154 0.0004053104394753595\n",
      "1401 1.0863252188792103 0.0004048849791218294\n",
      "1501 1.0622602235816885 0.000404460855797593\n",
      "1601 1.1195327076129615 0.00040403806251449327\n",
      "1701 1.2447982146404684 0.00040361659233540054\n",
      "1801 1.2607627244724426 0.0004031964383737348\n",
      "1901 1.278331945562968 0.00040277759379299307\n",
      "2001 1.025594950420782 0.0004023600518062819\n",
      "2101 1.115274733179831 0.0004019438056758561\n",
      "2201 1.1167924739420414 0.0004015288487126612\n",
      "2301 1.0995151306560729 0.000401115174275883\n",
      "2401 1.1547567544039339 0.00040070277577250023\n",
      "2501 1.1590442548913416 0.00040029164665684384\n",
      "2601 1.1047132272506133 0.00039988178043016053\n",
      "2701 1.0620037270709872 0.00039947317064018093\n",
      "2801 1.0939110746548977 0.00039906581088069363\n",
      "2901 0.9693534299731255 0.0003986596947911227\n",
      "3001 1.2754340882529505 0.0003982548160561108\n",
      "3101 2.0011723663365046 0.0003978511684051071\n",
      "3201 1.1672507325711194 0.0003974487456119586\n",
      "3301 0.8547125565819442 0.00039704754149450736\n",
      "3401 1.0948779656609986 0.00039664754991419163\n",
      "3501 1.1662541554399013 0.0003962487647756509\n",
      "3601 1.242357063729287 0.00039585118002633614\n",
      "3701 1.142975198366912 0.000395454789656124\n",
      "3801 1.2055247909738682 0.0003950595876969351\n",
      "3901 1.2129587295930833 0.0003946655682223565\n",
      "4001 1.239052205113694 0.0003942727253472687\n",
      "4101 1.1285374546132516 0.0003938810532274764\n",
      "4201 1.3123125492420513 0.00039349054605934306\n",
      "4301 1.2088057449145708 0.00039310119807943006\n",
      "4401 1.0897887839237228 0.00039271300356413926\n",
      "4501 1.2131327866227366 0.00039232595682935973\n",
      "4601 0.9877158605959266 0.000391940052230118\n",
      "4701 1.2145014504967548 0.0003915552841602323\n",
      "4801 1.1513765653944574 0.0003911716470519708\n",
      "4901 1.1751896993955597 0.0003907891353757127\n",
      "5001 1.1771067604749987 0.0003904077436396139\n",
      "5101 1.3224336538114585 0.0003900274663892758\n",
      "5201 1.355893952131737 0.0003896482982074174\n",
      "5301 1.1996791153214872 0.0003892702337135512\n",
      "5401 1.206806231304654 0.0003888932675636631\n",
      "5501 1.280991047504358 0.0003885173944498942\n",
      "5601 0.9168080711970106 0.0003881426091002278\n",
      "5701 1.1924245768459514 0.0003877689062781782\n",
      "5801 1.1940782586170826 0.0003873962807824836\n",
      "5901 1.2784329915011767 0.0003870247274468023\n",
      "6001 1.1448089724872261 0.00038665424113941134\n",
      "6101 1.1181278676813236 0.0003862848167629092\n",
      "6201 1.2686003018752672 0.00038591644925392126\n",
      "6301 1.0795898175565526 0.000385549133582808\n",
      "1 1.199639980099164 0.00038523407955521927\n",
      "101 1.0967486363369972 0.00038486870703897236\n",
      "201 1.2039306252845563 0.00038450437215947677\n",
      "301 1.106695241353009 0.0003841410700146326\n",
      "401 1.0133273621067929 0.00038377879573470126\n",
      "501 1.1209047999582253 0.0003834175444820315\n",
      "601 1.073817516444251 0.00038305731145078797\n",
      "701 1.1002086726948619 0.00038269809186668256\n",
      "801 1.044247637852095 0.00038233988098670897\n",
      "901 1.1538543488713913 0.00038198267409887953\n",
      "1001 1.1638364950194955 0.00038162646652196454\n",
      "1101 1.1222136826545466 0.00038127125360523515\n",
      "1201 1.0936923912668135 0.0003809170307282081\n",
      "1301 1.0790816302178428 0.0003805637933003932\n",
      "1401 1.0973517978854943 0.0003802115367610436\n",
      "1501 1.3543376340385294 0.00037986025657890806\n",
      "1601 1.0638599513913505 0.0003795099482519871\n",
      "1701 1.095864930888638 0.0003791606073072896\n",
      "1801 1.3785420355270617 0.00037881222930059356\n",
      "1901 1.0656100183841772 0.0003784648098162084\n",
      "2001 1.083574770949781 0.0003781183444667399\n",
      "2101 1.7192173111798184 0.0003777728288928577\n",
      "2201 1.090653446619399 0.0003774282587630644\n",
      "2301 1.1122783308528597 0.00037708462977346826\n",
      "2401 0.95696423901245 0.0003767419376475568\n",
      "2501 1.2160079335735645 0.0003764001781359734\n",
      "2601 1.2086039673304185 0.00037605934701629616\n",
      "2701 1.0832101813866757 0.00037571944009281874\n",
      "2801 1.013074157119263 0.00037538045319633314\n",
      "2901 1.0823434699559584 0.00037504238218391556\n",
      "3001 1.1612105248786975 0.0003747052229387128\n",
      "3101 0.9126896761590615 0.0003743689713697328\n",
      "3201 1.3341417479550728 0.00037403362341163505\n",
      "3301 0.9600269035436213 0.0003736991750245252\n",
      "3401 1.1916928359714802 0.0003733656221937497\n",
      "3501 1.4976106537505984 0.0003730329609296942\n",
      "3601 1.1840611910447478 0.0003727011872675824\n",
      "3701 1.3150727476167958 0.0003723702972672783\n",
      "3801 1.221188339870423 0.00037204028701308904\n",
      "3901 1.2026138188084587 0.0003717111526135708\n",
      "4001 1.3793403076779214 0.00037138289020133557\n",
      "4101 1.0772470782976598 0.0003710554959328607\n",
      "4201 1.0168679640773917 0.0003707289659882998\n",
      "4301 1.1685322925950459 0.00037040329657129513\n",
      "4401 1.1224966624286026 0.00037007848390879306\n",
      "4501 1.0253048577578738 0.00036975452425085955\n",
      "4601 1.2101853350850433 0.00036943141387049916\n",
      "4701 1.050108958443161 0.0003691091490634741\n",
      "4801 1.2717001468117815 0.00036878772614812674\n",
      "4901 1.1231291381409392 0.00036846714146520227\n",
      "5001 1.1141781120968517 0.00036814739137767423\n",
      "5101 0.9994667179416865 0.00036782847227057074\n",
      "5201 1.3557415267750912 0.0003675103805508032\n",
      "5301 1.504937146051816 0.0003671931126469962\n",
      "5401 1.0444834220979828 0.00036687666500931896\n",
      "5501 1.0159150707913795 0.0003665610341093186\n",
      "5601 1.4691102042561397 0.00036624621643975515\n",
      "5701 1.2679062836105004 0.0003659322085144373\n",
      "5801 1.1070539963402553 0.0003656190068680607\n",
      "5901 1.2043958652066067 0.0003653066080560474\n",
      "6001 1.1217296464601532 0.00036499500865438625\n",
      "6101 1.2132740695233224 0.00036468420525947586\n",
      "6201 1.452793362134571 0.00036437419448796804\n",
      "6301 1.0731251265387982 0.00036406497297661317\n",
      "1 1.1998733360724145 0.00036379350826718935\n",
      "101 1.1498671763110906 0.0003634857615296514\n",
      "201 1.1022596344992053 0.00036317879447701637\n",
      "301 1.0011312331771478 0.00036287260382257964\n",
      "401 1.0373523531015962 0.0003625671862990008\n",
      "501 1.0644397677097004 0.000362262538658157\n",
      "601 1.1183270415349398 0.000361958657670998\n",
      "701 0.9439565273642074 0.00036165554012740277\n",
      "801 1.0483181119780056 0.0003613531828360362\n",
      "901 1.10791165966657 0.00036105158262420917\n",
      "1001 0.9895736404578201 0.00036075073633773743\n",
      "1101 1.0942751504917396 0.00036045064084080426\n",
      "1201 1.0274720180314034 0.0003601512930158222\n",
      "1301 1.049829078532639 0.0003598526897632977\n",
      "1401 1.0599873741302872 0.00035955482800169595\n",
      "1501 1.1110646773013286 0.00035925770466730756\n",
      "1601 1.1195740707335062 0.0003589613167141159\n",
      "1701 1.1175601889844984 0.0003586656611136664\n",
      "1801 1.27650167158572 0.00035837073485493607\n",
      "1901 1.1153064398095012 0.000358076534944205\n",
      "2001 1.4756392514391337 0.000357783058404929\n",
      "2101 0.8967400579713285 0.0003574903022776121\n",
      "2201 1.0554919667192735 0.0003571982636196827\n",
      "2301 1.1484477042686194 0.000356906939505368\n",
      "2401 1.0967925000586547 0.00035661632702557175\n",
      "2501 1.275310180048109 0.0003563264232877516\n",
      "2601 1.084061863599345 0.00035603722541579873\n",
      "2701 1.076280384673737 0.00035574873054991784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801 1.200010517553892 0.0003554609358465082\n",
      "2901 1.059172057226533 0.000355173838478046\n",
      "3001 1.0261963941156864 0.000354887435632968\n",
      "3101 0.9737269952893257 0.0003546017245155551\n",
      "3201 1.109491402952699 0.0003543167023458187\n",
      "3301 1.0379895093501545 0.0003540323663593864\n",
      "3401 1.0210047286818735 0.00035374871380738974\n",
      "3501 1.3062616163679195 0.0003534657419563522\n",
      "3601 1.1297821700572968 0.00035318344808807914\n",
      "3701 1.095454223890556 0.0003529018294995477\n",
      "3801 1.0263875699602067 0.00035262088350279793\n",
      "3901 1.0200049103004858 0.00035234060742482575\n",
      "4001 1.139240143907955 0.00035206099860747537\n",
      "4101 1.185085133272878 0.00035178205440733397\n",
      "4201 1.0887831579057092 0.0003515037721956263\n",
      "4301 1.7533796445081862 0.000351226149358111\n",
      "4401 1.1149956742010545 0.00035094918329497705\n",
      "4501 1.1544227619015146 0.0003506728714207421\n",
      "4601 1.1121961249154992 0.00035039721116415036\n",
      "4701 1.0929120010696352 0.0003501221999680728\n",
      "4801 1.118996370700188 0.000349847835289407\n",
      "4901 0.9860638748505153 0.00034957411459897886\n",
      "5001 1.004449057742022 0.0003493010353814441\n",
      "5101 1.2927988782757893 0.00034902859513519165\n",
      "5201 0.98420900356723 0.0003487567913722472\n",
      "5301 1.1210692654858576 0.000348485621618178\n",
      "5401 1.163566045666812 0.00034821508341199766\n",
      "5501 1.17701395630138 0.0003479451743060731\n",
      "5601 1.0291424575298151 0.00034767589186603104\n",
      "5701 1.2543358486509533 0.0003474072336706659\n",
      "5801 1.2299754508348997 0.00034713919731184855\n",
      "5901 1.1936145080917413 0.0003468717803944353\n",
      "6001 0.9138605990447104 0.00034660498053617827\n",
      "6101 1.1037570714397589 0.00034633879536763624\n",
      "6201 1.04157008238235 0.00034607322253208626\n",
      "6301 1.1919174637878314 0.0003458082596854357\n",
      "1 0.9797578346915543 0.00034557559511139293\n",
      "101 0.9891712895187084 0.00034531177274179953\n",
      "201 1.1815550197497942 0.00034504855367990236\n",
      "301 1.1358435613219626 0.0003447859356297997\n",
      "401 1.0717519058380276 0.000344523916307803\n",
      "501 1.172375235328218 0.00034426249344235384\n",
      "601 1.0603420100815129 0.00034400166477394084\n",
      "701 1.0992282917286502 0.000343741428055018\n",
      "801 1.04559408465866 0.0003434817810499231\n",
      "901 1.0248865495998416 0.0003432227215347973\n",
      "1001 1.0598302248690743 0.0003429642472975047\n",
      "1101 1.0938185814011376 0.0003427063561375535\n",
      "1201 1.291374852447916 0.000342449045866017\n",
      "1301 1.0285806620959193 0.0003421923143054557\n",
      "1401 1.17992360109929 0.0003419361592898398\n",
      "1501 0.9615428688703105 0.0003416805786644727\n",
      "1601 1.2486475716141285 0.0003414255702859144\n",
      "1701 0.9794061238644645 0.0003411711320219065\n",
      "1801 1.1059001302346587 0.00034091726175129706\n",
      "1901 0.9131126408465207 0.00034066395736396637\n",
      "2001 0.9930663524428383 0.0003404112167607534\n",
      "2101 1.0812218267819844 0.0003401590378533823\n",
      "2201 1.0715046060213353 0.0003399074185643907\n",
      "2301 1.1185214965953492 0.00033965635682705713\n",
      "2401 1.05721441534115 0.00033940585058533\n",
      "2501 1.0936700437378022 0.00033915589779375693\n",
      "2601 1.07034515045234 0.00033890649641741454\n",
      "2701 1.0813248989579733 0.00033865764443183875\n",
      "2801 1.0510470166627783 0.0003384093398229561\n",
      "2901 0.9623011860530823 0.0003381615805870148\n",
      "3001 1.1494725269731134 0.00033791436473051725\n",
      "3101 1.2335324875239166 0.0003376676902701525\n",
      "3201 1.0398004396120086 0.00033742155523272933\n",
      "3301 1.0589452146668918 0.0003371759576551101\n",
      "3401 1.084106142167002 0.00033693089558414497\n",
      "3501 1.0406659920408856 0.0003366863670766065\n",
      "3601 0.9325393754988909 0.00033644237019912526\n",
      "3701 1.0507557194505353 0.0003361989030281253\n",
      "3801 0.945562198292464 0.0003359559636497606\n",
      "3901 1.0489263081690297 0.0003357135501598519\n",
      "4001 1.0528855019947514 0.00033547166066382383\n",
      "4101 1.1952165458351374 0.0003352302932766432\n",
      "4201 1.0958911241032183 0.00033498944612275674\n",
      "4301 1.077680416405201 0.0003347491173360301\n",
      "4401 1.2972540742484853 0.0003345093050596873\n",
      "4501 1.039087069220841 0.0003342700074462501\n",
      "4601 0.9597453814931214 0.00033403122265747876\n",
      "4701 1.1728105103829876 0.00033379294886431207\n",
      "4801 1.1258336059836438 0.0003335551842468092\n",
      "4901 1.0011352995352354 0.0003333179269940906\n",
      "5001 0.9672558718448272 0.00033308117530428074\n",
      "5101 1.0522874586749822 0.0003328449273844502\n",
      "5201 1.0063609674107283 0.0003326091814505589\n",
      "5301 1.0480196221014921 0.00033237393572739917\n",
      "5401 1.0445173801199417 0.00033213918844854004\n",
      "5501 1.417743748796056 0.00033190493785627127\n",
      "5601 1.0902981872641249 0.000331671182201548\n",
      "5701 1.0301871165866032 0.00033143791974393625\n",
      "5801 1.4090074983541854 0.00033120514875155805\n",
      "5901 1.1904211936052889 0.0003309728675010378\n",
      "6001 1.1052953382313717 0.0003307410742774485\n",
      "6101 1.133972127106972 0.00033050976737425853\n",
      "6201 1.4820798086614104 0.00033027894509327907\n",
      "6301 1.1476092239608988 0.00033004860574461153\n",
      "1 1.0870586273958907 0.00032984630526377065\n",
      "101 1.00110832543578 0.00032961686928176145\n",
      "201 0.9876259700540686 0.0003293879114110055\n",
      "301 1.2413200094233616 0.0003291594299932822\n",
      "401 0.9424758276436478 0.00032893142337841173\n",
      "501 1.0926933737646323 0.00032870388992420444\n",
      "601 1.0706572374765528 0.0003284768279964114\n",
      "701 1.0879125816572923 0.00032825023596867546\n",
      "801 1.262531905740616 0.0003280241122224816\n",
      "901 1.050877535046311 0.0003277984551471088\n",
      "1001 1.135452825037646 0.0003275732631395822\n",
      "1101 1.0099836179433623 0.0003273485346046242\n",
      "1201 1.1641883124248125 0.0003271242679546084\n",
      "1301 1.0249766572378576 0.00032690046160951133\n",
      "1401 1.216345368164184 0.0003266771139968662\n",
      "1501 1.4009095890432945 0.00032645422355171653\n",
      "1601 0.9214687866624445 0.00032623178871657\n",
      "1701 1.050587208737852 0.0003260098079413526\n",
      "1801 1.0106142781150993 0.0003257882796833635\n",
      "1901 0.9388233295176178 0.00032556720240723\n",
      "2001 1.1458081254386343 0.0003253465745848626\n",
      "2101 1.0818304931126477 0.00032512639469541087\n",
      "2201 0.8635702040046453 0.0003249066612252194\n",
      "2301 1.0180434776411857 0.00032468737266778394\n",
      "2401 1.0467977939988486 0.0003244685275237081\n",
      "2501 1.083000476603047 0.0003242501243006605\n",
      "2601 1.1069668279960752 0.00032403216151333166\n",
      "2701 1.086160118225962 0.00032381463768339173\n",
      "2801 1.0924749624973629 0.0003235975513394485\n",
      "2901 1.009744831302669 0.00032338090101700554\n",
      "3001 0.9085385013604537 0.0003231646852584205\n",
      "3101 1.1706041378201917 0.00032294890261286426\n",
      "3201 1.032271361502353 0.00032273355163627964\n",
      "3301 1.2584509218577296 0.00032251863089134133\n",
      "3401 1.2436874122358859 0.000322304138947415\n",
      "3501 1.0451832013077365 0.0003220900743805179\n",
      "3601 1.0900762653473066 0.00032187643577327854\n",
      "3701 1.1192542002827395 0.00032166322171489793\n",
      "3801 1.00253647408681 0.0003214504308011099\n",
      "3901 1.2160937447333708 0.0003212380616341424\n",
      "4001 1.0416435159859248 0.0003210261128226793\n",
      "4101 1.3598752447869629 0.00032081458298182156\n",
      "4201 1.0555532689650136 0.0003206034707330495\n",
      "4301 1.1295962483854964 0.00032039277470418526\n",
      "4401 0.9410244208120275 0.0003201824935293548\n",
      "4501 0.8939700378105044 0.00031997262584895135\n",
      "4601 0.908640876179561 0.000319763170309598\n",
      "4701 1.128680162204546 0.0003195541255641112\n",
      "4801 1.0497526655672118 0.00031934549027146444\n",
      "4901 1.0289937005145475 0.0003191372630967521\n",
      "5001 0.9918764412868768 0.0003189294427111535\n",
      "5101 1.2255524442298338 0.0003187220277918973\n",
      "5201 1.3292681298672733 0.0003185150170222263\n",
      "5301 0.878005885053426 0.00031830840909136197\n",
      "5401 1.0740752452165907 0.00031810220269447\n",
      "5501 1.0994656120092259 0.00031789639653262544\n",
      "5601 1.159670107124839 0.0003176909893127784\n",
      "5701 0.8859041188843548 0.0003174859797477199\n",
      "5801 1.084522244927939 0.00031728136655604814\n",
      "5901 1.4824702723776682 0.0003170771484621346\n",
      "6001 1.230977819112013 0.0003168733241960908\n",
      "6101 1.0119965468620649 0.00031666989249373517\n",
      "6201 1.1002646164814678 0.00031646685209656003\n",
      "6301 1.1440792203939054 0.00031626420175169897\n",
      "1 1.0551144047021808 0.0003160882122689669\n",
      "101 1.0408390048833098 0.00031588628797931984\n",
      "201 1.0153360446565785 0.0003156847501772966\n",
      "301 1.01748421555385 0.0003154835976315582\n",
      "401 1.1267781729111448 0.0003152828291162512\n",
      "501 1.0327468327741371 0.0003150824434109757\n",
      "601 0.9960314880299848 0.0003148824393007546\n",
      "701 1.0631007702104398 0.00031468281557600267\n",
      "801 1.0372768385277595 0.00031448357103249544\n",
      "901 1.031023440795252 0.0003142847044713392\n",
      "1001 0.9323838343843818 0.0003140862146989404\n",
      "1101 0.850510573014617 0.0003138881005269756\n",
      "1201 1.0943628003296908 0.0003136903607723615\n",
      "1301 1.0844742289336864 0.00031349299425722566\n",
      "1401 1.4024500491796061 0.00031329599980887637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 1.1463393379817717 0.00031309937625977405\n",
      "1601 1.0650637014914537 0.0003129031224475018\n",
      "1701 1.1410465109511279 0.00031270723721473664\n",
      "1801 1.0148204645956866 0.0003125117194092209\n",
      "1901 0.9743651752360165 0.0003123165678837336\n",
      "2001 0.990701739974611 0.00031212178149606226\n",
      "2101 1.1128740338463103 0.00031192735910897496\n",
      "2201 1.5508626039809315 0.0003117332995901923\n",
      "2301 1.01486110695987 0.00031153960181235955\n",
      "2401 0.9611194784665713 0.0003113462646530196\n",
      "2501 1.0847897573257796 0.0003111532869945851\n",
      "2601 1.2803321699175285 0.00031096066772431187\n",
      "2701 1.0769891024538083 0.0003107684057342714\n",
      "2801 1.0039243546780199 0.00031057649992132457\n",
      "2901 1.0824949400266632 0.00031038494918709473\n",
      "3001 0.9644128995714709 0.00031019375243794144\n",
      "3101 0.9868561172188492 0.00031000290858493437\n",
      "3201 1.0903575613629073 0.00030981241654382685\n",
      "3301 1.2633273452374851 0.0003096222752350304\n",
      "3401 1.0088038056419464 0.000309432483583589\n",
      "3501 1.1047235757578164 0.0003092430405191533\n",
      "3601 1.163446888080216 0.00030905394497595545\n",
      "3701 1.018205283649877 0.00030886519589278384\n",
      "3801 1.0408570388099179 0.00030867679221295824\n",
      "3901 1.0657447287230752 0.00030848873288430483\n",
      "4001 0.930832964291767 0.0003083010168591314\n",
      "4101 1.4587874389944773 0.00030811364309420327\n",
      "4201 1.1227497690124437 0.0003079266105507184\n",
      "4301 1.1970081577601377 0.0003077399181942835\n",
      "4401 1.1083186157047749 0.00030755356499488986\n",
      "4501 1.0473160178516991 0.00030736754992688985\n",
      "4601 1.0928417469840497 0.0003071818719689727\n",
      "4701 1.0073067757184617 0.00030699653010414117\n",
      "4801 1.1523846584532293 0.00030681152331968824\n",
      "4901 1.1988015054084826 0.0003066268506071739\n",
      "5001 1.036811558995396 0.00030644251096240176\n",
      "5101 1.0675939484208357 0.0003062585033853964\n",
      "5201 1.0752534797684348 0.00030607482688038056\n",
      "5301 1.1183025861246279 0.0003058914804557523\n",
      "5401 1.0365500289481133 0.0003057084631240629\n",
      "5501 1.129350705537945 0.00030552577390199393\n",
      "5601 1.06671357084997 0.0003053434118103358\n",
      "5701 1.0859052878222428 0.0003051613758739652\n",
      "5801 1.0723684425465763 0.00030497966512182316\n",
      "5901 1.0603833084605867 0.0003047982785868937\n",
      "6001 1.0581669194652932 0.0003046172153061818\n",
      "6101 1.2675022858026068 0.0003044364743206923\n",
      "6201 1.0536423055746127 0.0003042560546754083\n",
      "6301 1.2465427681345318 0.00030407595541926996\n",
      "1 0.8441335130482912 0.00030391413923858634\n",
      "101 0.9350836968515068 0.00030373464611573535\n",
      "201 1.0421846130630001 0.0003035554706461405\n",
      "301 1.008769184758421 0.0003033766118939749\n",
      "401 0.9787611065548845 0.000303198068927267\n",
      "501 1.0469113910803571 0.00030301984081788013\n",
      "601 1.0306272330635693 0.00030284192664149214\n",
      "701 0.8391264111269265 0.00030266432547757535\n",
      "801 0.9852646276758605 0.00030248703640937665\n",
      "901 0.9640902730752714 0.00030231005852389745\n",
      "1001 1.1280414578068303 0.00030213339091187405\n",
      "1101 0.9939612101297826 0.0003019570326677579\n",
      "1201 1.0867023059399799 0.00030178098288969626\n",
      "1301 1.0411206257122103 0.00030160524067951265\n",
      "1401 1.0711584523378406 0.0003014298051426879\n",
      "1501 1.0796883448783774 0.0003012546753883405\n",
      "1601 1.027666941517964 0.00030107985052920836\n",
      "1701 1.082986782770604 0.00030090532968162913\n",
      "1801 1.1074479344606516 0.0003007311119655219\n",
      "1901 1.1305997944582487 0.0003005571965043686\n",
      "2001 1.345339710366943 0.0003003835824251953\n",
      "2101 1.001590578132891 0.00030021026885855383\n",
      "2201 1.0054450589232147 0.0003000372549385033\n",
      "2301 1.0041432639409322 0.00029986453980259265\n",
      "2401 1.0640304164699046 0.00029969212259184163\n",
      "2501 1.1201181028736755 0.0002995200024507235\n",
      "2601 0.8765224074013531 0.00029934817852714696\n",
      "2701 1.0152945614827331 0.0002991766499724386\n",
      "2801 1.2956223709957158 0.0002990054159413251\n",
      "2901 1.097986907014274 0.0002988344755919157\n",
      "3001 1.2291080165232415 0.00029866382808568526\n",
      "3101 1.1140619127836544 0.0002984934725874564\n",
      "3201 1.0722678579004423 0.0002983234082653825\n",
      "3301 1.03946516571159 0.0002981536342909311\n",
      "3401 0.9876702070032479 0.0002979841498388662\n",
      "3501 0.8540887358831242 0.00029781495408723205\n",
      "3601 0.9894529741141014 0.00029764604621733594\n",
      "3701 1.0710785342380404 0.000297477425413732\n",
      "3801 1.2710673977526312 0.00029730909086420423\n",
      "3901 1.0929949116252828 0.0002971410417597504\n",
      "4001 1.1222996068827342 0.0002969732772945655\n",
      "4101 1.164539644116303 0.00029680579666602566\n",
      "4201 1.033734397671651 0.000296638599074672\n",
      "4301 1.093015514779836 0.0002964716837241944\n",
      "4401 1.0481613585725427 0.0002963050498214161\n",
      "4501 1.0937337041832507 0.00029613869657627706\n",
      "4601 1.1815662420112858 0.000295972623201819\n",
      "4701 1.1428916620570817 0.0002958068289141693\n",
      "4801 1.1009264337189961 0.0002956413129325257\n",
      "4901 0.8777621657354757 0.00029547607447914055\n",
      "5001 1.0156730558082927 0.00029531111277930595\n",
      "5101 0.9942513670539483 0.00029514642706133804\n",
      "5201 1.1302866424011881 0.00029498201655656206\n",
      "5301 1.0087051462905947 0.0002948178804992971\n",
      "5401 1.0660143050336046 0.0002946540181268415\n",
      "5501 1.0107977577135898 0.00029449042867945755\n",
      "5601 1.0777363086963305 0.0002943271114003569\n",
      "5701 0.856828257907182 0.00029416406553568584\n",
      "5801 1.1287360956775956 0.0002940012903345107\n",
      "5901 1.366358119645156 0.00029383878504880313\n",
      "6001 1.0964845723065082 0.00029367654893342604\n",
      "6101 1.34646458978159 0.00029351458124611887\n",
      "6201 1.2197050878312439 0.0002933528812474836\n",
      "6301 1.0830936049751472 0.0002931914482009704\n",
      "1 1.2537849597129025 0.00029304477550482497\n",
      "101 0.9655292083625682 0.00029288385030021\n",
      "201 1.1372923650196753 0.0002927231899204302\n",
      "301 0.9451354363700375 0.0002925627936399378\n",
      "401 0.9661671929707154 0.00029240266073596516\n",
      "501 1.0162687979172915 0.0002922427904885108\n",
      "601 0.9551542007829994 0.0002920831821803257\n",
      "701 0.9841917234880384 0.0002919238350969\n",
      "801 1.061543255826109 0.00029176474852644945\n",
      "901 0.985908080736408 0.0002916059217599022\n",
      "1001 1.0337736615701942 0.0002914473540908853\n",
      "1101 0.9899544979416532 0.0002912890448157118\n",
      "1201 1.1052642236463726 0.00029113099323336726\n",
      "1301 0.9609193275682628 0.00029097319864549706\n",
      "1401 1.0143777604680508 0.0002908156603563932\n",
      "1501 1.0131031578639522 0.0002906583776729816\n",
      "1601 1.0399196342332289 0.00029050134990480915\n",
      "1701 1.3567781529854983 0.00029034457636403104\n",
      "1801 1.1145936762022757 0.0002901880563653981\n",
      "1901 1.0984270876506343 0.0002900317892262443\n",
      "2001 1.0226630433771788 0.00028987577426647405\n",
      "2101 1.1856945900362916 0.0002897200108085499\n",
      "2201 1.125245438015554 0.00028956449817748025\n",
      "2301 1.126162831991678 0.00028940923570080693\n",
      "2401 0.971063018507266 0.00028925422270859307\n",
      "2501 0.8773902256507427 0.00028909945853341086\n",
      "2601 0.8763325407635421 0.0002889449425103295\n",
      "2701 1.1415061227562546 0.0002887906739769035\n",
      "2801 1.052460735765635 0.0002886366522731603\n",
      "2901 1.5205009760629764 0.00028848287674158846\n",
      "3001 0.9414957111439435 0.0002883293467271265\n",
      "3101 0.9435001520323567 0.0002881760615771502\n",
      "3201 1.1403545759221743 0.0002880230206414618\n",
      "3301 1.053262686386006 0.00028787022327227786\n",
      "3401 1.0832857484929264 0.000287717668824218\n",
      "3501 1.1019753235159442 0.00028756535665429354\n",
      "3601 1.055358653771691 0.0002874132861218958\n",
      "3701 1.0278627741499804 0.00028726145658878504\n",
      "3801 1.0083879251906183 0.0002871098674190792\n",
      "3901 1.0555589499126654 0.0002869585179792425\n",
      "4001 1.0509156602493022 0.00028680740763807453\n",
      "4101 1.0385481148259714 0.0002866565357666993\n",
      "4201 1.0417402729653986 0.0002865059017385537\n",
      "4301 1.082753369351849 0.0002863555049293774\n",
      "4401 1.0459589868987678 0.0002862053447172013\n",
      "4501 1.0592919351911405 0.00028605542048233684\n",
      "4601 1.1034397517796606 0.0002859057316073656\n",
      "4701 1.1311876591207692 0.00028575627747712837\n",
      "4801 1.070465801298269 0.00028560705747871445\n",
      "4901 1.199700104945805 0.00028545807100145134\n",
      "5001 1.2379299406893551 0.00028530931743689397\n",
      "5101 1.1045849512156565 0.00028516079617881457\n",
      "5201 0.9958119990806154 0.000285012506623192\n",
      "5301 1.620139996672151 0.00028486444816820157\n",
      "5401 1.0613090786646353 0.0002847166202142048\n",
      "5501 1.175794189737644 0.0002845690221637393\n",
      "5601 1.1241089710965753 0.00028442165342150834\n",
      "5701 1.160597581154434 0.000284274513394371\n",
      "5801 1.0310369414401066 0.0002841276014913322\n",
      "5901 0.9960121748881647 0.0002839809171235324\n",
      "6001 0.9698299318188219 0.00028383445970423817\n",
      "6101 1.1415085992775857 0.0002836882286488319\n",
      "6201 1.0641657677479088 0.0002835422233748022\n",
      "6301 1.0828722650112468 0.00028339644330173413\n"
     ]
    }
   ],
   "source": [
    "#weight = torch.ones(len(TGT.vocab))\n",
    "#weight[pad_idx] = 0\n",
    "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "35JC6i9QUgSB"
   },
   "outputs": [],
   "source": [
    "1 10.825187489390373 6.987712429686844e-07\n",
    "101 9.447168171405792 3.56373333914029e-05\n",
    "201 7.142856806516647 7.057589553983712e-05\n",
    "301 6.237934365868568 0.00010551445768827134\n",
    "401 5.762486848048866 0.00014045301983670557\n",
    "501 5.415792358107865 0.00017539158198513977\n",
    "601 5.081815680023283 0.000210330144133574\n",
    "701 4.788327748770826 0.00024526870628200823\n",
    "801 4.381739928154275 0.0002802072684304424\n",
    "901 4.55433791608084 0.00031514583057887664\n",
    "1001 4.911875109748507 0.0003500843927273108\n",
    "1101 4.0579032292589545 0.0003850229548757451\n",
    "1201 4.2276234351193125 0.0004199615170241793\n",
    "1301 3.932735869428143 0.00045490007917261356\n",
    "1401 3.8179439397063106 0.0004898386413210477\n",
    "1501 3.3608515430241823 0.000524777203469482\n",
    "1601 3.832796103321016 0.0005597157656179162\n",
    "1701 2.907085266895592 0.0005946543277663504\n",
    "1801 3.5280659823838505 0.0006295928899147847\n",
    "1901 2.895841649500653 0.0006645314520632189\n",
    "2001 3.273784235585481 0.000699470014211653\n",
    "2101 3.181488689899197 0.0007344085763600873\n",
    "2201 3.4151616653980454 0.0007693471385085215\n",
    "2301 3.4343731447588652 0.0008042857006569557\n",
    "2401 3.0505455391539726 0.0008392242628053899\n",
    "2501 2.8089329147478566 0.0008741628249538242\n",
    "2601 2.7827929875456903 0.0009091013871022583\n",
    "2701 2.4428516102489084 0.0009440399492506926\n",
    "2801 2.4015486147254705 0.0009789785113991267\n",
    "2901 2.3568112018401735 0.001013917073547561\n",
    "3001 2.6349758653668687 0.0010488556356959952\n",
    "3101 2.5981983028614195 0.0010837941978444295\n",
    "3201 2.666826274838968 0.0011187327599928637\n",
    "3301 3.0092043554177508 0.0011536713221412978\n",
    "3401 2.4580375660589198 0.0011886098842897321\n",
    "3501 2.586465588421561 0.0012235484464381662\n",
    "3601 2.5663993963389657 0.0012584870085866006\n",
    "3701 2.9430236657499336 0.0012934255707350347\n",
    "3801 2.464644919440616 0.001328364132883469\n",
    "3901 2.7124062888276512 0.0013633026950319032\n",
    "4001 2.646443709731102 0.0013971932312809247\n",
    "4101 2.7294750874862075 0.001380057517579748\n",
    "4201 2.1295202329056337 0.0013635372009002666\n",
    "4301 2.596563663915731 0.001347596306985731\n",
    "4401 2.1265982036820787 0.0013322017384983986\n",
    "4501 2.3880532500334084 0.0013173229858148\n",
    "4601 2.6129120760888327 0.0013029318725783852\n",
    "4701 2.2873719420749694 0.001289002331178292\n",
    "4801 2.4949760700110346 0.0012755102040816328\n",
    "4901 2.496607314562425 0.001262433067573089\n",
    "5001 2.1889712483389303 0.0012497500749750088\n",
    "5101 1.8677761815488338 0.0012374418168536253\n",
    "5201 2.2992054556962103 0.0012254901960784316\n",
    "5301 2.664361578106707 0.0012138783159049418\n",
    "5401 2.705850490485318 0.0012025903795063202\n",
    "5501 2.581445264921058 0.0011916115995949978\n",
    "5601 2.2480602325085783 0.0011809281169581616\n",
    "5701 1.9289666265249252 0.0011705269268863989\n",
    "5801 2.4863578918157145 0.0011603958126073107\n",
    "5901 2.632946971571073 0.0011505232849492607\n",
    "6001 2.496141305891797 0.0011408985275576757\n",
    "6101 2.6422974687084206 0.0011315113470699342\n",
    "6201 2.448802186456305 0.0011223521277270118"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "The Annotated \"Attention is All You Need\".ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
