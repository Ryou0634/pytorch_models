{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numberish\n",
    "\n",
    "### Grammar\n",
    "#### translate English words into their lengths\n",
    "ex) 'I', 'often', 'wonder', 'if', 'it', 'might', 'be', 'X' -> '1', '5', '6', '2', '2', '5', '2', '1'\n",
    "\n",
    "#### swith (n | n%3 == 0) th words with (n | n%3 == 1) th words\n",
    "ex) '1', '5', '6', '2', '2', '5', '2', '1' -> '5', '1', '6', '2', '2', '5', '1', '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary:  2069\n",
      "Numberish vocabulary:  12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from my_utils import Dictionary, DataLoader, Trainer\n",
    "\n",
    "en_dict = Dictionary()\n",
    "num_dict = Dictionary(['<BOS>', '<EOS>'])\n",
    "\n",
    "dataset = []\n",
    "with open('./eng-num.txt', 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i == 1000: break\n",
    "        eng, num = line[:-1].split('\\t')\n",
    "        eng_list = eng.split()\n",
    "        num_list = list(num)\n",
    "        dataset.append((eng_list, num_list))\n",
    "        for w in eng_list:\n",
    "            en_dict.add_word(w)\n",
    "        for n in num_list:\n",
    "            num_dict.add_word(n)\n",
    "\n",
    "print('English vocabulary: ', len(en_dict))\n",
    "print('Numberish vocabulary: ', len(num_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numericalize(seq, dictionary):\n",
    "    return [dictionary(token) for token in seq]\n",
    "numericalized = [(numericalize(eng, en_dict), numericalize(num, num_dict)) for eng, num in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_models.models import MLP, LSTMEncoder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embed_size, src_vocab_size, tgt_vocab_size, tgt_BOS, tgt_EOS, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.encoder = LSTMEncoder(embed_size, src_vocab_size, bidirectional=False, num_layers=num_layers)\n",
    "        self.decoder = LSTMEncoder(embed_size, tgt_vocab_size, bidirectional=False, num_layers=num_layers)\n",
    "        self.out_mlp = MLP(dims=[embed_size, tgt_vocab_size])\n",
    "        \n",
    "        self.tgt_BOS = tgt_BOS\n",
    "        self.tgt_EOS = tgt_EOS\n",
    "\n",
    "    def fit(self, inputs, targets, optimizer):\n",
    "        # encoding\n",
    "        _, enc_hiddens = self.encoder.forward(inputs)\n",
    "        # decoding\n",
    "        BOS_targets = self._append_BOS(targets)\n",
    "        decoded, _ = self.decoder.forward(BOS_targets, enc_hiddens)\n",
    "        decoded = self._flatten_and_unpad(decoded)\n",
    "        # predicting\n",
    "        targets_EOS = self._append_EOS_flatten(targets)\n",
    "        loss = self.out_mlp.fit(decoded, targets_EOS, optimizer)\n",
    "        return loss\n",
    "    \n",
    "    def _append_BOS(self, targets):\n",
    "        BOS_targets = [torch.cat((torch.tensor([self.tgt_BOS]), target)) for target in targets]\n",
    "        return BOS_targets\n",
    "        \n",
    "    def _append_EOS_flatten(self, targets):\n",
    "        EOS_targets = [torch.cat((target, torch.tensor([self.tgt_EOS]))) for target in targets]\n",
    "        return torch.cat(EOS_targets)\n",
    "    \n",
    "    def _flatten_and_unpad(self, decoded):\n",
    "        decoded = decoded.view(-1, self.decoder.embed_size) # (batch * seq_len, embed_size)\n",
    "        decoded = torch.stack([tensor for tensor in decoded if not torch.tensor(float('-inf')) in tensor], dim=0)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Device =====\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch_models.models import LSTMEncoder, DotAttn\n",
    "from torch_models.utils import seq2seq, get_device\n",
    "\n",
    "device = get_device()\n",
    "train_loader = DataLoader(numericalized, batch_size = 4, trans_func=seq2seq)\n",
    "model = Seq2Seq(50, len(en_dict), len(num_dict), tgt_BOS=num_dict('<BOS>'), tgt_EOS=num_dict('<EOS>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  \tloss: 2.084116714000702\t\n",
      "epoch 1  \tloss: 2.0596342821121216\t\n",
      "epoch 2  \tloss: 2.0508489060401915\t\n",
      "epoch 3  \tloss: 2.034707717895508\t\n",
      "epoch 4  \tloss: 2.028004758358002\t\n",
      "epoch 5  \tloss: 2.0195835165977476\t\n",
      "epoch 6  \tloss: 2.025053680419922\t\n",
      "epoch 7  \tloss: 2.031168773174286\t\n",
      "epoch 8  \tloss: 2.0377994151115417\t\n",
      "epoch 9  \tloss: 2.0425978140830994\t\n"
     ]
    }
   ],
   "source": [
    "from my_utils import Trainer, EvaluatorC\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# evaluator = EvaluatorC(model, test_loader)\n",
    "\n",
    "trainer = Trainer(model, train_loader)\n",
    "trainer.train(optimizer, max_epoch=1,\n",
    "              evaluator=None, score_monitor=None, show_log=True, hook_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
